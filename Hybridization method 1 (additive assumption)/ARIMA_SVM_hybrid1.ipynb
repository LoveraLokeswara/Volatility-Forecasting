{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab450646",
      "metadata": {},
      "outputs": [],
      "source": [
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import math\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import random\n",
        "from datetime import datetime as dt\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import warnings\n",
        "import tensorflow as tf\n",
        "import random\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set display options for better output\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', 1000)\n",
        "\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65c62479",
      "metadata": {},
      "source": [
        "# Prepare Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc98076b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data Download Configuration\n",
        "# S&P 500: January 1, 2002 to December 31, 2023\n",
        "# Bitcoin: January 1, 2015 to December 31, 2023\n",
        "\n",
        "# Define date ranges\n",
        "sp500_start = \"2002-01-01\"\n",
        "sp500_end = \"2023-12-31\"\n",
        "bitcoin_start = \"2015-01-01\"\n",
        "bitcoin_end = \"2023-12-31\"\n",
        "\n",
        "print(\"Downloading S&P 500 data...\")\n",
        "sp500_data = yf.download(\"^GSPC\", start=sp500_start, end=sp500_end, progress=False)\n",
        "\n",
        "print(\"Downloading Bitcoin data...\")\n",
        "bitcoin_data = yf.download(\"BTC-USD\", start=bitcoin_start, end=bitcoin_end, progress=False)\n",
        "\n",
        "# Display basic information about downloaded data\n",
        "print(f\"\\nS&P 500 Data Shape: {sp500_data.shape}\")\n",
        "print(f\"S&P 500 Date Range: {sp500_data.index.min()} to {sp500_data.index.max()}\")\n",
        "print(f\"Total S&P 500 observations: {len(sp500_data)}\")\n",
        "\n",
        "print(f\"\\nBitcoin Data Shape: {bitcoin_data.shape}\")\n",
        "print(f\"Bitcoin Date Range: {bitcoin_data.index.min()} to {bitcoin_data.index.max()}\")\n",
        "print(f\"Total Bitcoin observations: {len(bitcoin_data)}\")\n",
        "\n",
        "sp500_data.columns = sp500_data.columns.set_levels(['Adj Close' if x == 'Close' else x for x in sp500_data.columns.levels[0]], level=0)\n",
        "\n",
        "bitcoin_data.columns = bitcoin_data.columns.set_levels(['Adj Close' if x == 'Close' else x for x in bitcoin_data.columns.levels[0]], level=0)\n",
        "\n",
        "\n",
        "print(\"\\nVerification - S&P 500 has Adj Close:\", ('Adj Close', '^GSPC') in sp500_data.columns)\n",
        "print(\"Verification - Bitcoin has Adj Close:\", ('Adj Close', 'BTC-USD') in bitcoin_data.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a7cba81",
      "metadata": {},
      "outputs": [],
      "source": [
        "sp500_data['Log_Returns'] = np.log(sp500_data['Adj Close'] / sp500_data['Adj Close'].shift(1))\n",
        "\n",
        "bitcoin_data['Log_Returns'] = np.log(bitcoin_data['Adj Close'] / bitcoin_data['Adj Close'].shift(1))\n",
        "\n",
        "sp500_clean = sp500_data.dropna()\n",
        "bitcoin_clean = bitcoin_data.dropna()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13049831",
      "metadata": {},
      "outputs": [],
      "source": [
        "statistics_data = {\n",
        "    'Observations': [len(sp500_clean), len(bitcoin_clean)],\n",
        "    'Mean_Daily_Return': [sp500_clean['Log_Returns'].mean(), bitcoin_clean['Log_Returns'].mean()],\n",
        "    'Standard_Deviation': [sp500_clean['Log_Returns'].std(), bitcoin_clean['Log_Returns'].std()],\n",
        "    'Minimum_Return': [sp500_clean['Log_Returns'].min(), bitcoin_clean['Log_Returns'].min()],\n",
        "    'Maximum_Return': [sp500_clean['Log_Returns'].max(), bitcoin_clean['Log_Returns'].max()],\n",
        "    'Skewness': [sp500_clean['Log_Returns'].skew(), bitcoin_clean['Log_Returns'].skew()],\n",
        "    'Kurtosis': [sp500_clean['Log_Returns'].kurtosis(), bitcoin_clean['Log_Returns'].kurtosis()],\n",
        "    'Infinite_Values': [np.isinf(sp500_clean['Log_Returns']).sum(), np.isinf(bitcoin_clean['Log_Returns']).sum()],\n",
        "    'NaN_Values': [sp500_clean['Log_Returns'].isnull().sum(), bitcoin_clean['Log_Returns'].isnull().sum()]\n",
        "}\n",
        "\n",
        "# Create DataFrame with asset names as index\n",
        "returns_statistics = pd.DataFrame(statistics_data, index=['S&P_500', 'Bitcoin'])\n",
        "\n",
        "returns_statistics.T\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "851e64d7",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data Visualization and Final Verification\n",
        "\n",
        "# Create visualizations\n",
        "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "# S&P 500 Price Series\n",
        "ax1.plot(sp500_clean.index, sp500_clean['Adj Close'])\n",
        "ax1.set_title('S&P 500 Adjusted Close Price (2002-2023)')\n",
        "ax1.set_ylabel('Price ($)')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Bitcoin Price Series\n",
        "ax2.plot(bitcoin_clean.index, bitcoin_clean['Adj Close'])\n",
        "ax2.set_title('Bitcoin Price (2015-2023)')\n",
        "ax2.set_ylabel('Price ($)')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# S&P 500 Log Returns\n",
        "ax3.plot(sp500_clean.index, sp500_clean['Log_Returns'])\n",
        "ax3.set_title('S&P 500 Logarithmic Returns')\n",
        "ax3.set_ylabel('Log Returns')\n",
        "ax3.set_xlabel('Date')\n",
        "ax3.axhline(y=0, color='red', linestyle='--', alpha=0.5)\n",
        "ax3.grid(True, alpha=0.3)\n",
        "\n",
        "# Bitcoin Log Returns\n",
        "ax4.plot(bitcoin_clean.index, bitcoin_clean['Log_Returns'])\n",
        "ax4.set_title('Bitcoin Logarithmic Returns')\n",
        "ax4.set_ylabel('Log Returns')\n",
        "ax4.set_xlabel('Date')\n",
        "ax4.axhline(y=0, color='red', linestyle='--', alpha=0.5)\n",
        "ax4.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Display sample data\n",
        "print(\"\\n=== SAMPLE DATA PREVIEW ===\\n\")\n",
        "print(\"S&P 500 Data (First 5 rows):\")\n",
        "print(sp500_clean[['Open', 'High', 'Low', 'Adj Close', 'Volume', 'Log_Returns']].head())\n",
        "\n",
        "print(\"\\nBitcoin Data (First 5 rows):\")\n",
        "print(bitcoin_clean[['Open', 'High', 'Low', 'Adj Close', 'Volume', 'Log_Returns']].head())\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d28b9ba",
      "metadata": {},
      "source": [
        "Stationary Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7fb862a5",
      "metadata": {},
      "outputs": [],
      "source": [
        "from statsmodels.tsa.stattools import adfuller\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "\n",
        "\n",
        "sp500_adf_test = adfuller(sp500_clean['Log_Returns'])\n",
        "# Output the results\n",
        "print('ADF Statistic: %f' % sp500_adf_test[0])\n",
        "print('p-value: %f' % sp500_adf_test[1])\n",
        "\n",
        "plot_acf(sp500_clean['Log_Returns'], lags=40)\n",
        "plot_pacf(sp500_clean['Log_Returns'], lags=40)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e56d93e5",
      "metadata": {},
      "outputs": [],
      "source": [
        "bitcoin_adf_test = adfuller(bitcoin_clean['Log_Returns'])\n",
        "# Output the results\n",
        "print('ADF Statistic: %f' % bitcoin_adf_test[0])\n",
        "print('p-value: %f' % bitcoin_adf_test[1])\n",
        "\n",
        "plot_acf(bitcoin_clean['Log_Returns'], lags=40)\n",
        "plot_pacf(bitcoin_clean['Log_Returns'], lags=40)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "702b8166",
      "metadata": {},
      "source": [
        "# Cross Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d56fe58",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Time Series Cross-Validation Implementation\n",
        "# Novel 3-fold cross-validation scheme with rolling windows\n",
        "\n",
        "import matplotlib.dates as mdates\n",
        "from datetime import datetime, timedelta\n",
        "from dateutil.relativedelta import relativedelta\n",
        "\n",
        "# print(\"=== TIME SERIES CROSS-VALIDATION IMPLEMENTATION ===\\n\")\n",
        "\n",
        "def create_sp500_cv_splits(data, start_date=None):\n",
        "    if start_date is None:\n",
        "        start_date = data.index.min()\n",
        "    \n",
        "    cv_splits = []\n",
        "    window_start = start_date\n",
        "    \n",
        "    while True:\n",
        "        # Define window boundaries\n",
        "        train_start = window_start\n",
        "        train_end = train_start + relativedelta(years=3) - timedelta(days=1)\n",
        "        \n",
        "        # Validation periods (8, 16, 24 months)\n",
        "        val_start = train_end + timedelta(days=1)\n",
        "        val1_end = val_start + relativedelta(months=8) - timedelta(days=1)  # 8 months\n",
        "        val2_end = val_start + relativedelta(months=16) - timedelta(days=1) # 16 months  \n",
        "        val3_end = val_start + relativedelta(months=24) - timedelta(days=1) # 24 months (2 years)\n",
        "        \n",
        "        # Test period (1 year)\n",
        "        test_start = val3_end + timedelta(days=1)\n",
        "        test_end = test_start + relativedelta(years=1) - timedelta(days=1)\n",
        "        \n",
        "        # Check if we have enough data\n",
        "        # if test_end > data.index.max():\n",
        "        if test_end.year > 2024:\n",
        "            break\n",
        "            \n",
        "        # Create splits for this window\n",
        "        train_data = data[(data.index >= train_start) & (data.index <= train_end)]\n",
        "        \n",
        "        # Three validation folds\n",
        "        val1_data = data[(data.index >= val_start) & (data.index <= val1_end)]\n",
        "        val2_data = data[(data.index >= val_start) & (data.index <= val2_end)]\n",
        "        val3_data = data[(data.index >= val_start) & (data.index <= val3_end)]\n",
        "        \n",
        "        test_data = data[(data.index >= test_start) & (data.index <= test_end)]\n",
        "        \n",
        "        cv_splits.append({\n",
        "            'window_id': len(cv_splits) + 1,\n",
        "            'train': {\n",
        "                'data': train_data,\n",
        "                'start': train_start,\n",
        "                'end': train_end,\n",
        "                'size': len(train_data)\n",
        "            },\n",
        "            'validation': [\n",
        "                {\n",
        "                    'fold': 1,\n",
        "                    'data': val1_data,\n",
        "                    'start': val_start,\n",
        "                    'end': val1_end,\n",
        "                    'size': len(val1_data),\n",
        "                    'months': 8\n",
        "                },\n",
        "                {\n",
        "                    'fold': 2, \n",
        "                    'data': val2_data,\n",
        "                    'start': val_start,\n",
        "                    'end': val2_end,\n",
        "                    'size': len(val2_data),\n",
        "                    'months': 16\n",
        "                },\n",
        "                {\n",
        "                    'fold': 3,\n",
        "                    'data': val3_data,\n",
        "                    'start': val_start,\n",
        "                    'end': val3_end,\n",
        "                    'size': len(val3_data),\n",
        "                    'months': 24\n",
        "                }\n",
        "            ],\n",
        "            'test': {\n",
        "                'data': test_data,\n",
        "                'start': test_start,\n",
        "                'end': test_end,\n",
        "                'size': len(test_data)\n",
        "            }\n",
        "        })\n",
        "        \n",
        "        # Move window forward by 1 year\n",
        "        window_start += relativedelta(years=1)\n",
        "    \n",
        "    return cv_splits\n",
        "\n",
        "def create_bitcoin_cv_splits(data, start_date=None):\n",
        "    \n",
        "    if start_date is None:\n",
        "        # Start from a date that allows for proper window construction\n",
        "        start_date = datetime(2015, 1, 1)\n",
        "    \n",
        "    cv_splits = []\n",
        "    window_start = start_date\n",
        "    \n",
        "    # Define the testing period constraint\n",
        "    test_period_start = datetime(2018, 1, 1)\n",
        "    test_period_end = datetime(2023, 12, 31)\n",
        "    \n",
        "    while True:\n",
        "        # Define window boundaries\n",
        "        train_start = window_start\n",
        "        train_end = train_start + relativedelta(years=2) - timedelta(days=1)\n",
        "        \n",
        "        # Validation periods (4, 8, 12 months)\n",
        "        val_start = train_end + timedelta(days=1)\n",
        "        val1_end = val_start + relativedelta(months=4) - timedelta(days=1)  # 4 months\n",
        "        val2_end = val_start + relativedelta(months=8) - timedelta(days=1)  # 8 months\n",
        "        val3_end = val_start + relativedelta(months=12) - timedelta(days=1) # 12 months\n",
        "        \n",
        "        # Test period (6 months)\n",
        "        test_start = val3_end + timedelta(days=1)\n",
        "        test_end = test_start + relativedelta(months=6) - timedelta(days=1)\n",
        "        \n",
        "        # Check constraints\n",
        "        # if test_end > data.index.max() or test_end > test_period_end:\n",
        "        #     break\n",
        "        if test_end.year > 2023:\n",
        "            break\n",
        "        \n",
        "        # Only include windows where test period is within 2018-2023\n",
        "        if test_start < test_period_start:\n",
        "            window_start += relativedelta(months=6)\n",
        "            continue\n",
        "            \n",
        "        # Create splits for this window\n",
        "        train_data = data[(data.index >= train_start) & (data.index <= train_end)]\n",
        "        \n",
        "        # Three validation folds\n",
        "        val1_data = data[(data.index >= val_start) & (data.index <= val1_end)]\n",
        "        val2_data = data[(data.index >= val_start) & (data.index <= val2_end)]\n",
        "        val3_data = data[(data.index >= val_start) & (data.index <= val3_end)]\n",
        "        \n",
        "        test_data = data[(data.index >= test_start) & (data.index <= test_end)]\n",
        "        \n",
        "        cv_splits.append({\n",
        "            'window_id': len(cv_splits) + 1,\n",
        "            'train': {\n",
        "                'data': train_data,\n",
        "                'start': train_start,\n",
        "                'end': train_end,\n",
        "                'size': len(train_data)\n",
        "            },\n",
        "            'validation': [\n",
        "                {\n",
        "                    'fold': 1,\n",
        "                    'data': val1_data,\n",
        "                    'start': val_start,\n",
        "                    'end': val1_end,\n",
        "                    'size': len(val1_data),\n",
        "                    'months': 4\n",
        "                },\n",
        "                {\n",
        "                    'fold': 2,\n",
        "                    'data': val2_data, \n",
        "                    'start': val_start,\n",
        "                    'end': val2_end,\n",
        "                    'size': len(val2_data),\n",
        "                    'months': 8\n",
        "                },\n",
        "                {\n",
        "                    'fold': 3,\n",
        "                    'data': val3_data,\n",
        "                    'start': val_start,\n",
        "                    'end': val3_end,\n",
        "                    'size': len(val3_data),\n",
        "                    'months': 12\n",
        "                }\n",
        "            ],\n",
        "            'test': {\n",
        "                'data': test_data,\n",
        "                'start': test_start,\n",
        "                'end': test_end,\n",
        "                'size': len(test_data)\n",
        "            }\n",
        "        })\n",
        "        \n",
        "        # Move window forward by 6 months\n",
        "        window_start += relativedelta(months=6)\n",
        "    \n",
        "    return cv_splits\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67f7d807",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply Cross-Validation Schemes to Data\n",
        "\n",
        "# Generate S&P 500 cross-validation splits\n",
        "sp500_cv_splits = create_sp500_cv_splits(sp500_clean)\n",
        "\n",
        "\n",
        "# Generate Bitcoin cross-validation splits  \n",
        "bitcoin_cv_splits = create_bitcoin_cv_splits(bitcoin_clean)\n",
        "\n",
        "\n",
        "# Create summary DataFrames\n",
        "def create_cv_summary(cv_splits, asset_name):\n",
        "    summary_data = []\n",
        "    \n",
        "    for split in cv_splits:\n",
        "        # Add training data info\n",
        "        summary_data.append({\n",
        "            'Asset': asset_name,\n",
        "            'Window_ID': split['window_id'],\n",
        "            'Split_Type': 'Train',\n",
        "            'Fold': 'N/A',\n",
        "            'Start_Date': split['train']['start'].strftime('%Y-%m-%d'),\n",
        "            'End_Date': split['train']['end'].strftime('%Y-%m-%d'),\n",
        "            'Size': split['train']['size'],\n",
        "            'Duration_Months': 'N/A'\n",
        "        })\n",
        "        \n",
        "        # Add validation data info\n",
        "        for val_fold in split['validation']:\n",
        "            summary_data.append({\n",
        "                'Asset': asset_name,\n",
        "                'Window_ID': split['window_id'],\n",
        "                'Split_Type': 'Validation',\n",
        "                'Fold': val_fold['fold'],\n",
        "                'Start_Date': val_fold['start'].strftime('%Y-%m-%d'),\n",
        "                'End_Date': val_fold['end'].strftime('%Y-%m-%d'),\n",
        "                'Size': val_fold['size'],\n",
        "                'Duration_Months': val_fold['months']\n",
        "            })\n",
        "        \n",
        "        # Add test data info\n",
        "        summary_data.append({\n",
        "            'Asset': asset_name,\n",
        "            'Window_ID': split['window_id'],\n",
        "            'Split_Type': 'Test',\n",
        "            'Fold': 'N/A',\n",
        "            'Start_Date': split['test']['start'].strftime('%Y-%m-%d'),\n",
        "            'End_Date': split['test']['end'].strftime('%Y-%m-%d'),\n",
        "            'Size': split['test']['size'],\n",
        "            'Duration_Months': 'N/A'\n",
        "        })\n",
        "    \n",
        "    return pd.DataFrame(summary_data)\n",
        "\n",
        "# Create summary DataFrames\n",
        "sp500_cv_summary = create_cv_summary(sp500_cv_splits, 'S&P_500')\n",
        "bitcoin_cv_summary = create_cv_summary(bitcoin_cv_splits, 'Bitcoin')\n",
        "\n",
        "# Combined summary\n",
        "cv_summary_combined = pd.concat([sp500_cv_summary, bitcoin_cv_summary], ignore_index=True)\n",
        "\n",
        "\n",
        "# Display first few windows for each asset\n",
        "print(\"\\n=== S&P 500 CV WINDOWS (First set) ===\")\n",
        "for i, split in enumerate(sp500_cv_splits[:1]):\n",
        "    print(f\"\\nWindow {split['window_id']}:\")\n",
        "    print(f\"  Train: {split['train']['start'].strftime('%Y-%m-%d')} to {split['train']['end'].strftime('%Y-%m-%d')} ({split['train']['size']} obs)\")\n",
        "    print(f\"  Validation Folds:\")\n",
        "    for val_fold in split['validation']:\n",
        "        print(f\"    Fold {val_fold['fold']} ({val_fold['months']}mo): {val_fold['start'].strftime('%Y-%m-%d')} to {val_fold['end'].strftime('%Y-%m-%d')} ({val_fold['size']} obs)\")\n",
        "    print(f\"  Test: {split['test']['start'].strftime('%Y-%m-%d')} to {split['test']['end'].strftime('%Y-%m-%d')} ({split['test']['size']} obs)\")\n",
        "\n",
        "print(\"\\n=== BITCOIN CV WINDOWS (First set) ===\")\n",
        "for i, split in enumerate(bitcoin_cv_splits[:1]):\n",
        "    print(f\"\\nWindow {split['window_id']}:\")\n",
        "    print(f\"  Train: {split['train']['start'].strftime('%Y-%m-%d')} to {split['train']['end'].strftime('%Y-%m-%d')} ({split['train']['size']} obs)\")\n",
        "    print(f\"  Validation Folds:\")\n",
        "    for val_fold in split['validation']:\n",
        "        print(f\"    Fold {val_fold['fold']} ({val_fold['months']}mo): {val_fold['start'].strftime('%Y-%m-%d')} to {val_fold['end'].strftime('%Y-%m-%d')} ({val_fold['size']} obs)\")\n",
        "    print(f\"  Test: {split['test']['start'].strftime('%Y-%m-%d')} to {split['test']['end'].strftime('%Y-%m-%d')} ({split['test']['size']} obs)\")\n",
        "\n",
        "# Display summary statistics\n",
        "print(\"\\n=== CV SPLIT STATISTICS ===\")\n",
        "split_stats = cv_summary_combined.groupby(['Asset', 'Split_Type']).agg({\n",
        "    'Size': ['mean', 'std', 'min', 'max'],\n",
        "    'Window_ID': 'count'\n",
        "}).round(0)\n",
        "print(split_stats)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "573c91f9",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize Cross-Validation Scheme\n",
        "\n",
        "def plot_cv_timeline(cv_splits, asset_name, max_windows=8):\n",
        "    \n",
        "    fig, ax = plt.subplots(figsize=(16, max(6, len(cv_splits[:max_windows]) * 1.5)))\n",
        "    \n",
        "    # Colors for different split types\n",
        "    colors = {\n",
        "        'train': '#2E8B57',\n",
        "        'val_fold1': '#4169E1',\n",
        "        'val_fold2': '#1E90FF',\n",
        "        'val_fold3': '#87CEEB',\n",
        "        'test': '#DC143C'\n",
        "    }\n",
        "    \n",
        "    y_positions = []\n",
        "    \n",
        "    for i, split in enumerate(cv_splits[:max_windows]):\n",
        "        y_pos = len(cv_splits[:max_windows]) - i - 1\n",
        "        y_positions.append(y_pos)\n",
        "        \n",
        "        # Plot training period\n",
        "        ax.barh(y_pos, (split['train']['end'] - split['train']['start']).days, \n",
        "                left=split['train']['start'], height=0.6, \n",
        "                color=colors['train'], alpha=0.8, label='Train' if i == 0 else \"\")\n",
        "        \n",
        "        # Plot validation periods\n",
        "        val_colors = ['val_fold1', 'val_fold2', 'val_fold3']\n",
        "        for j, val_fold in enumerate(split['validation']):\n",
        "            ax.barh(y_pos + 0.1 + j*0.15, (val_fold['end'] - val_fold['start']).days,\n",
        "                    left=val_fold['start'], height=0.12,\n",
        "                    color=colors[val_colors[j]], alpha=0.8,\n",
        "                    label=f'Val Fold {j+1} ({val_fold[\"months\"]}mo)' if i == 0 else \"\")\n",
        "        \n",
        "        # Plot test period\n",
        "        ax.barh(y_pos, (split['test']['end'] - split['test']['start']).days,\n",
        "                left=split['test']['start'], height=0.6,\n",
        "                color=colors['test'], alpha=0.8, label='Test' if i == 0 else \"\")\n",
        "        \n",
        "        # Add window labels\n",
        "        ax.text(split['train']['start'], y_pos, f'W{split[\"window_id\"]}',\n",
        "                verticalalignment='center', fontsize=9, fontweight='bold')\n",
        "    \n",
        "    # Formatting\n",
        "    ax.set_ylim(-0.5, len(cv_splits[:max_windows]) - 0.5)\n",
        "    ax.set_ylabel('CV Windows (Newest to Oldest)', fontsize=12)\n",
        "    ax.set_xlabel('Time Period', fontsize=12)\n",
        "    ax.set_title(f'{asset_name} Cross-Validation Timeline\\n({len(cv_splits)} Total Windows, Showing First {min(max_windows, len(cv_splits))})', \n",
        "                fontsize=14, fontweight='bold')\n",
        "    \n",
        "    # Format x-axis\n",
        "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
        "    ax.xaxis.set_major_locator(mdates.YearLocator())\n",
        "    plt.xticks(rotation=45)\n",
        "    \n",
        "    # Add legend\n",
        "    ax.legend(loc='upper right', bbox_to_anchor=(1, 1), frameon=True, fancybox=True, shadow=True)\n",
        "    \n",
        "    # Add grid\n",
        "    ax.grid(True, alpha=0.3, axis='x')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    return fig, ax\n",
        "\n",
        "# Create a comprehensive comparison chart\n",
        "def create_cv_comparison_chart():\n",
        "    \"\"\"Create a side-by-side comparison of CV schemes\"\"\"\n",
        "    \n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
        "    \n",
        "    # S&P 500 scheme visualization\n",
        "    y_pos = 1\n",
        "    \n",
        "    # S&P 500 scheme\n",
        "    ax1.barh(y_pos, 3*365, left=0, height=0.6, color='#2E8B57', alpha=0.8, label='Train (3yr)')\n",
        "    ax1.barh(y_pos+0.1, 8*30, left=3*365, height=0.15, color='#4169E1', alpha=0.8, label='Val Fold 1 (8mo)')\n",
        "    ax1.barh(y_pos+0.25, 16*30, left=3*365, height=0.15, color='#1E90FF', alpha=0.8, label='Val Fold 2 (16mo)')\n",
        "    ax1.barh(y_pos+0.4, 24*30, left=3*365, height=0.15, color='#87CEEB', alpha=0.8, label='Val Fold 3 (24mo)')\n",
        "    ax1.barh(y_pos, 1*365, left=5*365, height=0.6, color='#DC143C', alpha=0.8, label='Test (1yr)')\n",
        "    \n",
        "    ax1.set_xlim(0, 6*365)\n",
        "    ax1.set_ylim(0.5, 1.8)\n",
        "    ax1.set_xlabel('Days')\n",
        "    ax1.set_title('S&P 500 CV Scheme\\n(6-year windows)', fontweight='bold')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Bitcoin scheme\n",
        "    ax2.barh(y_pos, 2*365, left=0, height=0.6, color='#2E8B57', alpha=0.8, label='Train (2yr)')\n",
        "    ax2.barh(y_pos+0.1, 4*30, left=2*365, height=0.15, color='#4169E1', alpha=0.8, label='Val Fold 1 (4mo)')\n",
        "    ax2.barh(y_pos+0.25, 8*30, left=2*365, height=0.15, color='#1E90FF', alpha=0.8, label='Val Fold 2 (8mo)')\n",
        "    ax2.barh(y_pos+0.4, 12*30, left=2*365, height=0.15, color='#87CEEB', alpha=0.8, label='Val Fold 3 (12mo)')\n",
        "    ax2.barh(y_pos, 6*30, left=3*365, height=0.6, color='#DC143C', alpha=0.8, label='Test (6mo)')\n",
        "    \n",
        "    ax2.set_xlim(0, 3.5*365)\n",
        "    ax2.set_ylim(0.5, 1.8)\n",
        "    ax2.set_xlabel('Days')\n",
        "    ax2.set_title('Bitcoin CV Scheme\\n(~3.5-year windows)', fontweight='bold')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.suptitle('Time Series Cross-Validation Scheme Comparison', fontsize=16, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    return fig\n",
        "\n",
        "# fig3 = create_cv_comparison_chart()\n",
        "# plt.show()\n",
        "\n",
        "# Create summary table\n",
        "cv_scheme_summary = pd.DataFrame({\n",
        "    'Asset': ['S&P 500', 'Bitcoin'],\n",
        "    'Total_Windows': [len(sp500_cv_splits), len(bitcoin_cv_splits)],\n",
        "    'Window_Length': ['6 years', '~3.5 years'],\n",
        "    'Train_Period': ['3 years', '2 years'],\n",
        "    'Validation_Folds': ['8/16/24 months', '4/8/12 months'],\n",
        "    'Test_Period': ['1 year', '6 months'],\n",
        "    'Window_Shift': ['1 year', '6 months'],\n",
        "    'Test_Coverage': [\n",
        "        f\"{sp500_cv_splits[0]['test']['start'].strftime('%Y-%m-%d')} to {sp500_cv_splits[-1]['test']['end'].strftime('%Y-%m-%d')}\",\n",
        "        f\"{bitcoin_cv_splits[0]['test']['start'].strftime('%Y-%m-%d')} to {bitcoin_cv_splits[-1]['test']['end'].strftime('%Y-%m-%d')}\"\n",
        "    ]\n",
        "})\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72dec6d8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Utility Functions for Cross-Validation Data Access\n",
        "\n",
        "def get_cv_data(cv_splits, window_id, fold=None, return_type='data'):\n",
        "    split = next((s for s in cv_splits if s['window_id'] == window_id), None)\n",
        "    if split is None:\n",
        "        raise ValueError(f\"Window ID {window_id} not found\")\n",
        "    \n",
        "    if return_type == 'train':\n",
        "        return split['train']['data']\n",
        "    elif return_type == 'test':\n",
        "        return split['test']['data']\n",
        "    elif return_type == 'validation':\n",
        "        if fold is None:\n",
        "            raise ValueError(\"Fold number must be specified for validation data\")\n",
        "        if fold not in [1, 2, 3]:\n",
        "            raise ValueError(\"Fold must be 1, 2, or 3\")\n",
        "        return split['validation'][fold-1]['data']\n",
        "    else:\n",
        "        return split\n",
        "\n",
        "# Example usage functions\n",
        "def demonstrate_cv_usage():\n",
        "    \n",
        "    # Example 1: Get training data from first S&P 500 window\n",
        "    train_data_sp500 = get_cv_data(sp500_cv_splits, window_id=1, return_type='train')\n",
        "    print(f\"S&P 500 Window 1 - Training data shape: {train_data_sp500.shape}\")\n",
        "    print(f\"Training period: {train_data_sp500.index.min()} to {train_data_sp500.index.max()}\")\n",
        "    \n",
        "    # Example 2: Get validation fold 2 data from first S&P 500 window  \n",
        "    val_data_sp500 = get_cv_data(sp500_cv_splits, window_id=1, fold=2, return_type='validation')\n",
        "    print(f\"\\nS&P 500 Window 1 - Validation Fold 2 shape: {val_data_sp500.shape}\")\n",
        "    print(f\"Validation period: {val_data_sp500.index.min()} to {val_data_sp500.index.max()}\")\n",
        "    \n",
        "    # Example 3: Get test data from first Bitcoin window\n",
        "    test_data_bitcoin = get_cv_data(bitcoin_cv_splits, window_id=1, return_type='test')\n",
        "    print(f\"\\nBitcoin Window 1 - Test data shape: {test_data_bitcoin.shape}\")\n",
        "    print(f\"Test period: {test_data_bitcoin.index.min()} to {test_data_bitcoin.index.max()}\")\n",
        "    \n",
        "    return train_data_sp500, val_data_sp500, test_data_bitcoin\n",
        "\n",
        "# Run demonstration\n",
        "sample_train, sample_val, sample_test = demonstrate_cv_usage()\n",
        "\n",
        "\n",
        "# Save CV splits for later use (optional)\n",
        "cv_implementation_summary = {\n",
        "    'sp500_cv_splits': sp500_cv_splits,\n",
        "    'bitcoin_cv_splits': bitcoin_cv_splits,\n",
        "    'sp500_summary': sp500_cv_summary,\n",
        "    'bitcoin_summary': bitcoin_cv_summary,\n",
        "    'combined_summary': cv_summary_combined,\n",
        "    'scheme_comparison': cv_scheme_summary\n",
        "}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37e106cf",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "7ce50c54",
      "metadata": {},
      "source": [
        "# ARIMA + SVM HYBRID\n",
        "\n",
        "## ARIMA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c3540c0",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ARIMA Model Implementation with AIC-based Selection\n",
        "# Modern approach replacing traditional Box-Jenkins methodology\n",
        "\n",
        "import itertools\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from statsmodels.tsa.stattools import adfuller, kpss\n",
        "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
        "import warnings\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from scipy import stats\n",
        "import time\n",
        "\n",
        "# Suppress convergence warnings for cleaner output\n",
        "warnings.filterwarnings('ignore', category=UserWarning)\n",
        "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
        "\n",
        "print(\"=== AIC-BASED ARIMA MODEL SELECTION IMPLEMENTATION ===\\n\")\n",
        "\n",
        "def find_optimal_arima_order(data, max_p=5, max_d=2, max_q=5, seasonal=False, \n",
        "                           information_criterion='aic', verbose=False):\n",
        "    \n",
        "    best_ic = np.inf\n",
        "    best_params = None\n",
        "    best_model = None\n",
        "    results_log = []\n",
        "    \n",
        "    # Create parameter grid\n",
        "    if seasonal:\n",
        "        # For seasonal ARIMA (not implemented in this study)\n",
        "        param_grid = itertools.product(range(max_p+1), range(max_d+1), range(max_q+1),\n",
        "                                     range(2), range(2), range(2), [12])\n",
        "    else:\n",
        "        # Standard ARIMA grid search\n",
        "        param_grid = itertools.product(range(max_p+1), range(max_d+1), range(max_q+1))\n",
        "    \n",
        "    total_combinations = (max_p+1) * (max_d+1) * (max_q+1)\n",
        "    \n",
        "    if verbose:\n",
        "        print(f\"Testing {total_combinations} ARIMA parameter combinations...\")\n",
        "        start_time = time.time()\n",
        "    \n",
        "    for i, params in enumerate(param_grid):\n",
        "        p, d, q = params[:3]\n",
        "        \n",
        "        # Skip if model is too simple (all parameters zero)\n",
        "        if p == 0 and d == 0 and q == 0:\n",
        "            continue\n",
        "            \n",
        "        try:\n",
        "            # Fit ARIMA model\n",
        "            model = ARIMA(data, order=(p, d, q))\n",
        "            fitted_model = model.fit()\n",
        "            \n",
        "            # Get information criterion value\n",
        "            if information_criterion.lower() == 'aic':\n",
        "                ic_value = fitted_model.aic\n",
        "            elif information_criterion.lower() == 'bic':\n",
        "                ic_value = fitted_model.bic  \n",
        "            elif information_criterion.lower() == 'hqic':\n",
        "                ic_value = fitted_model.hqic\n",
        "            else:\n",
        "                ic_value = fitted_model.aic\n",
        "            \n",
        "            # Store results\n",
        "            results_log.append({\n",
        "                'order': (p, d, q),\n",
        "                'aic': fitted_model.aic,\n",
        "                'bic': fitted_model.bic,\n",
        "                'hqic': fitted_model.hqic,\n",
        "                'llf': fitted_model.llf,\n",
        "                'converged': fitted_model.mle_retvals['converged'] if hasattr(fitted_model, 'mle_retvals') else True\n",
        "            })\n",
        "            \n",
        "            # Update best model if current is better\n",
        "            if ic_value < best_ic:\n",
        "                best_ic = ic_value\n",
        "                best_params = (p, d, q)\n",
        "                best_model = fitted_model\n",
        "                \n",
        "        except Exception as e:\n",
        "            # Log failed fits\n",
        "            results_log.append({\n",
        "                'order': (p, d, q),\n",
        "                'aic': np.nan,\n",
        "                'bic': np.nan,  \n",
        "                'hqic': np.nan,\n",
        "                'llf': np.nan,\n",
        "                'converged': False,\n",
        "                'error': str(e)\n",
        "            })\n",
        "            \n",
        "            if verbose and i % 10 == 0:\n",
        "                print(f\"Failed to fit ARIMA{params}: {str(e)[:50]}...\")\n",
        "                \n",
        "        if verbose and (i + 1) % 20 == 0:\n",
        "            elapsed = time.time() - start_time\n",
        "            progress = (i + 1) / total_combinations * 100\n",
        "            print(f\"Progress: {progress:.1f}% ({i+1}/{total_combinations}) | \"\n",
        "                  f\"Best so far: ARIMA{best_params} ({information_criterion.upper()}={best_ic:.4f})\")\n",
        "    \n",
        "    if verbose:\n",
        "        total_time = time.time() - start_time\n",
        "        print(f\"\\nGrid search completed in {total_time:.2f} seconds\")\n",
        "        print(f\"Best model: ARIMA{best_params} with {information_criterion.upper()}={best_ic:.4f}\")\n",
        "    \n",
        "    # Create results summary\n",
        "    results_df = pd.DataFrame(results_log)\n",
        "    successful_fits = results_df[results_df['converged'] == True]\n",
        "    \n",
        "    return {\n",
        "        'best_order': best_params,\n",
        "        'best_model': best_model,\n",
        "        'best_ic_value': best_ic,\n",
        "        'information_criterion': information_criterion,\n",
        "        'results_df': results_df,\n",
        "        'successful_fits': len(successful_fits),\n",
        "        'total_attempts': len(results_log),\n",
        "        'success_rate': len(successful_fits) / len(results_log) * 100\n",
        "    }\n",
        "\n",
        "def evaluate_arima_model(model, train_data, test_data, model_order):\n",
        "    # Generate forecasts\n",
        "    n_forecast = len(test_data)\n",
        "    forecast_result = model.get_forecast(steps=n_forecast)\n",
        "    forecasts = forecast_result.predicted_mean\n",
        "    forecast_ci = forecast_result.conf_int()\n",
        "    \n",
        "    # Calculate performance metrics\n",
        "    mse = mean_squared_error(test_data, forecasts)\n",
        "    rmse = np.sqrt(mse)\n",
        "    mae = mean_absolute_error(test_data, forecasts)\n",
        "    mape = np.mean(np.abs((test_data - forecasts) / test_data)) * 100\n",
        "    \n",
        "    # Calculate R² score for fair comparison with SVM\n",
        "    ss_res = np.sum((test_data - forecasts) ** 2)\n",
        "    ss_tot = np.sum((test_data - np.mean(test_data)) ** 2)\n",
        "    r2 = 1 - (ss_res / ss_tot) if ss_tot > 0 else 0\n",
        "    \n",
        "    # Direction accuracy (for returns)\n",
        "    direction_actual = np.sign(test_data.values[1:])\n",
        "    direction_forecast = np.sign(forecasts.values[1:])\n",
        "    direction_accuracy = np.mean(direction_actual == direction_forecast) * 100\n",
        "    \n",
        "    # Residual diagnostics\n",
        "    residuals = model.resid\n",
        "    \n",
        "    # Ljung-Box test for serial correlation in residuals\n",
        "    lb_test = acorr_ljungbox(residuals, lags=10, return_df=False)\n",
        "\n",
        "    # Normality test (Jarque-Bera)\n",
        "    jb_stat, jb_pvalue = stats.jarque_bera(residuals)\n",
        "    \n",
        "    # Heteroskedasticity test (simple approach)\n",
        "    residuals_squared = residuals ** 2\n",
        "    arch_stat, arch_pvalue = acorr_ljungbox(residuals_squared, lags=5, return_df=False)\n",
        "    \n",
        "    return {\n",
        "        'model_order': model_order,\n",
        "        'forecasts': forecasts,\n",
        "        'forecast_ci': forecast_ci,\n",
        "        'performance_metrics': {\n",
        "            'mse': mse,\n",
        "            'rmse': rmse, \n",
        "            'mae': mae,\n",
        "            'mape': mape,\n",
        "            'r2': r2,\n",
        "            'direction_accuracy': direction_accuracy\n",
        "        },\n",
        "        'diagnostic_tests': {\n",
        "            'ljung_box_stat': lb_test['lb_stat'].iloc[-1],\n",
        "            'ljung_box_pvalue': lb_test['lb_pvalue'].iloc[-1],\n",
        "            'jarque_bera_stat': jb_stat,\n",
        "            'jarque_bera_pvalue': jb_pvalue,\n",
        "            'arch_stat': arch_stat[-1] if isinstance(arch_stat, np.ndarray) else arch_stat,\n",
        "            'arch_pvalue': arch_pvalue[-1] if isinstance(arch_pvalue, np.ndarray) else arch_pvalue\n",
        "        },\n",
        "        'residuals': residuals\n",
        "    }\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f9a512d",
      "metadata": {},
      "source": [
        "## HYBRID ARIMA-SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32200bb3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cross-Validation Integration for ARIMA Model Selection\n",
        "# Implement the complete methodology with hyperparameter optimization\n",
        "\n",
        "def run_arima_cross_validation(cv_splits, data_clean, asset_name, max_p=3, max_d=2, max_q=3, \n",
        "                              information_criterion='aic', verbose=True):\n",
        "    \n",
        "    print(f\"\\n=== {asset_name.upper()} ARIMA CROSS-VALIDATION ===\")\n",
        "    print(f\"Running AIC-based model selection across {len(cv_splits)} windows...\")\n",
        "    print(f\"Parameter search space: p∈[0,{max_p}], d∈[0,{max_d}], q∈[0,{max_q}]\")\n",
        "    print(\"-\" * 80)\n",
        "    \n",
        "    all_results = []\n",
        "    model_selection_summary = []\n",
        "    \n",
        "    for window_idx, split in enumerate(cv_splits):\n",
        "        window_id = split['window_id']\n",
        "        \n",
        "        if verbose:\n",
        "            print(f\"\\n Processing Window {window_id}/{len(cv_splits)}...\")\n",
        "            print(f\"   Train: {split['train']['start'].strftime('%Y-%m-%d')} to {split['train']['end'].strftime('%Y-%m-%d')} ({split['train']['size']} obs)\")\n",
        "            print(f\"   Test:  {split['test']['start'].strftime('%Y-%m-%d')} to {split['test']['end'].strftime('%Y-%m-%d')} ({split['test']['size']} obs)\")\n",
        "        \n",
        "        # Extract data\n",
        "        train_data = split['train']['data']['Log_Returns']\n",
        "        test_data = split['test']['data']['Log_Returns']\n",
        "        \n",
        "        # STEP 1: Model Selection using Training Data\n",
        "        if verbose:\n",
        "            print(f\"    Model selection using {information_criterion.upper()} criterion...\")\n",
        "        \n",
        "        selection_result = find_optimal_arima_order(\n",
        "            train_data, \n",
        "            max_p=max_p, \n",
        "            max_d=max_d, \n",
        "            max_q=max_q,\n",
        "            information_criterion=information_criterion,\n",
        "            verbose=False  # Keep individual window selection quiet\n",
        "        )\n",
        "        \n",
        "        if selection_result['best_model'] is None:\n",
        "            print(f\"    Failed to find suitable model for Window {window_id}\")\n",
        "            continue\n",
        "        \n",
        "        best_order = selection_result['best_order']\n",
        "        \n",
        "        # STEP 2: Hyperparameter Validation using Validation Folds\n",
        "        if verbose:\n",
        "            print(f\"    Validating ARIMA{best_order} across 3 validation folds...\")\n",
        "        \n",
        "        validation_scores = []\n",
        "        \n",
        "        for val_fold in split['validation']:\n",
        "            fold_num = val_fold['fold']\n",
        "            val_data = val_fold['data']['Log_Returns']\n",
        "            \n",
        "            try:\n",
        "                # Fit model on training data and evaluate on validation fold\n",
        "                val_model = ARIMA(train_data, order=best_order).fit()\n",
        "                val_forecasts = val_model.get_forecast(steps=len(val_data)).predicted_mean\n",
        "                val_rmse = np.sqrt(mean_squared_error(val_data, val_forecasts))\n",
        "                validation_scores.append(val_rmse)\n",
        "                \n",
        "            except Exception as e:\n",
        "                if verbose:\n",
        "                    print(f\"       Validation fold {fold_num} failed: {str(e)[:50]}...\")\n",
        "                validation_scores.append(np.inf)\n",
        "        \n",
        "        avg_validation_rmse = np.mean(validation_scores)\n",
        "        \n",
        "        # STEP 3: Final Model Training and Out-of-Sample Evaluation\n",
        "        if verbose:\n",
        "            print(f\"    Final evaluation on test data...\")\n",
        "        \n",
        "        try:\n",
        "            # Re-fit the model on training data\n",
        "            final_model = ARIMA(train_data, order=best_order).fit()\n",
        "            \n",
        "            # Evaluate on test data\n",
        "            evaluation = evaluate_arima_model(final_model, train_data, test_data, best_order)\n",
        "            \n",
        "            # Store comprehensive results\n",
        "            window_result = {\n",
        "                'window_id': window_id,\n",
        "                'asset': asset_name,\n",
        "                'train_period': f\"{split['train']['start'].strftime('%Y-%m-%d')} to {split['train']['end'].strftime('%Y-%m-%d')}\",\n",
        "                'test_period': f\"{split['test']['start'].strftime('%Y-%m-%d')} to {split['test']['end'].strftime('%Y-%m-%d')}\",\n",
        "                'train_size': split['train']['size'],\n",
        "                'test_size': split['test']['size'],\n",
        "                'best_order': best_order,\n",
        "                'model_selection': selection_result,\n",
        "                'validation_scores': validation_scores,\n",
        "                'avg_validation_rmse': avg_validation_rmse,\n",
        "                'evaluation': evaluation,\n",
        "                'final_model': final_model\n",
        "            }\n",
        "            \n",
        "            all_results.append(window_result)\n",
        "            \n",
        "            # Summary for quick reference\n",
        "            model_selection_summary.append({\n",
        "                'Window': window_id,\n",
        "                'Best_Order': f\"ARIMA{best_order}\",\n",
        "                'AIC': selection_result['best_ic_value'],\n",
        "                'Validation_RMSE': avg_validation_rmse,\n",
        "                'Test_RMSE': evaluation['performance_metrics']['rmse'],\n",
        "                'Test_MAE': evaluation['performance_metrics']['mae'],\n",
        "                'Test_R2': evaluation['performance_metrics']['r2'],\n",
        "                'Direction_Accuracy': evaluation['performance_metrics']['direction_accuracy'],\n",
        "                'Ljung_Box_p': evaluation['diagnostic_tests']['ljung_box_pvalue']\n",
        "            })\n",
        "            \n",
        "            if verbose:\n",
        "                print(f\"    ARIMA{best_order}: Test RMSE={evaluation['performance_metrics']['rmse']:.6f}, \"\n",
        "                      f\"Direction Acc={evaluation['performance_metrics']['direction_accuracy']:.1f}%\")\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"    Final evaluation failed for Window {window_id}: {str(e)}\")\n",
        "            continue\n",
        "    \n",
        "    # Create summary DataFrame\n",
        "    summary_df = pd.DataFrame(model_selection_summary)\n",
        "    \n",
        "    # Calculate overall performance statistics\n",
        "    if len(summary_df) > 0:\n",
        "        performance_summary = {\n",
        "            'total_windows': len(cv_splits),\n",
        "            'successful_windows': len(summary_df),\n",
        "            'success_rate': len(summary_df) / len(cv_splits) * 100,\n",
        "            'avg_test_rmse': summary_df['Test_RMSE'].mean(),\n",
        "            'std_test_rmse': summary_df['Test_RMSE'].std(),\n",
        "            'avg_test_mae': summary_df['Test_MAE'].mean(),\n",
        "            'avg_r2': summary_df['Test_R2'].mean(),\n",
        "            'avg_direction_accuracy': summary_df['Direction_Accuracy'].mean(),\n",
        "            'avg_validation_rmse': summary_df['Validation_RMSE'].mean(),\n",
        "            'most_common_order': summary_df['Best_Order'].mode().iloc[0] if len(summary_df) > 0 else None\n",
        "        }\n",
        "    else:\n",
        "        performance_summary = None\n",
        "    \n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"{asset_name.upper()} ARIMA CROSS-VALIDATION COMPLETE\")\n",
        "    print(f\"{'='*80}\")\n",
        "    \n",
        "    if performance_summary:\n",
        "        print(f\" Successfully processed {performance_summary['successful_windows']}/{performance_summary['total_windows']} windows\")\n",
        "        print(f\" Average Test RMSE: {performance_summary['avg_test_rmse']:.6f} ± {performance_summary['std_test_rmse']:.6f}\")\n",
        "        print(f\" Average Direction Accuracy: {performance_summary['avg_direction_accuracy']:.2f}%\")\n",
        "        print(f\" Most Common Model: {performance_summary['most_common_order']}\")\n",
        "    else:\n",
        "        print(\" No successful model fits achieved\")\n",
        "    \n",
        "    return {\n",
        "        'asset_name': asset_name,\n",
        "        'all_results': all_results,\n",
        "        'summary_df': summary_df,\n",
        "        'performance_summary': performance_summary,\n",
        "        'methodology': {\n",
        "            'approach': 'AIC-based automated selection',\n",
        "            'information_criterion': information_criterion,\n",
        "            'parameter_space': f'p∈[0,{max_p}], d∈[0,{max_d}], q∈[0,{max_q}]',\n",
        "            'cross_validation': '3-fold temporal validation',\n",
        "            'evaluation_metric': 'Out-of-sample RMSE and direction accuracy'\n",
        "        }\n",
        "    }\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "756b4ea5",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "def create_lagged_features_from_residuals(residuals, lookback=10):\n",
        "    X, y = [], []\n",
        "    for i in range(lookback, len(residuals)):\n",
        "        X.append(residuals[i-lookback:i])\n",
        "        y.append(residuals[i])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "\n",
        "def build_residual_svm_model(kernel='rbf', C=1.0, epsilon=0.1, gamma='scale'):\n",
        "    model = SVR(\n",
        "        kernel=kernel,\n",
        "        C=C,\n",
        "        epsilon=epsilon,\n",
        "        gamma=gamma,\n",
        "        cache_size=500\n",
        "    )\n",
        "    return model\n",
        "\n",
        "\n",
        "def run_hybrid_arima_svm_cv(cv_splits, data_clean, asset_name,\n",
        "                             max_p=3, max_d=1, max_q=3,\n",
        "                             lookback=10, kernel='rbf', C=1.0, \n",
        "                             epsilon=0.1, gamma='scale', verbose=True):\n",
        "    \n",
        "    print(f\"\\n{'='*100}\")\n",
        "    print(f\"HYBRID ARIMA-SVM CROSS-VALIDATION: {asset_name.upper()}\")\n",
        "    print(f\"{'='*100}\")\n",
        "    print(f\"Architecture: ARIMA (linear) + SVM on residuals (non-linear)\")\n",
        "    print(f\"ARIMA space: p∈[0,{max_p}], d∈[0,{max_d}], q∈[0,{max_q}]\")\n",
        "    print(f\"SVM config: lookback={lookback}, kernel={kernel}, C={C}, epsilon={epsilon}\")\n",
        "    print(f\"Total windows: {len(cv_splits)}\")\n",
        "    print(\"-\" * 100)\n",
        "    \n",
        "    all_results = []\n",
        "    scaler = StandardScaler()\n",
        "    \n",
        "    for window_idx, split in enumerate(cv_splits):\n",
        "        window_id = split['window_id']\n",
        "        \n",
        "        if verbose:\n",
        "            print(f\"\\n Window {window_id}/{len(cv_splits)}\")\n",
        "            print(f\"   Train: {split['train']['start'].strftime('%Y-%m-%d')} to {split['train']['end'].strftime('%Y-%m-%d')} ({split['train']['size']} obs)\")\n",
        "            print(f\"   Test:  {split['test']['start'].strftime('%Y-%m-%d')} to {split['test']['end'].strftime('%Y-%m-%d')} ({split['test']['size']} obs)\")\n",
        "        \n",
        "        try:\n",
        "            # Extract data\n",
        "            train_data = split['train']['data']['Log_Returns'].values\n",
        "            test_data = split['test']['data']['Log_Returns'].values\n",
        "            \n",
        "            # STEP 1: FIT ARIMA MODEL (Linear Component)\n",
        "            if verbose:\n",
        "                print(f\"   [1/4]  Fitting ARIMA model...\")\n",
        "            \n",
        "            # Find optimal ARIMA order\n",
        "            from statsmodels.tsa.arima.model import ARIMA\n",
        "            best_aic = np.inf\n",
        "            best_order = None\n",
        "            best_model = None\n",
        "            \n",
        "            for p in range(max_p + 1):\n",
        "                for d in range(max_d + 1):\n",
        "                    for q in range(max_q + 1):\n",
        "                        try:\n",
        "                            model = ARIMA(train_data, order=(p, d, q))\n",
        "                            fitted_model = model.fit()\n",
        "                            if fitted_model.aic < best_aic:\n",
        "                                best_aic = fitted_model.aic\n",
        "                                best_order = (p, d, q)\n",
        "                                best_model = fitted_model\n",
        "                        except:\n",
        "                            continue\n",
        "            \n",
        "            if best_model is None:\n",
        "                print(f\"    Failed to fit ARIMA for window {window_id}\")\n",
        "                continue\n",
        "            \n",
        "            if verbose:\n",
        "                print(f\"        Best ARIMA{best_order}, AIC={best_aic:.2f}\")\n",
        "            \n",
        "            # STEP 2: CALCULATE RESIDUALS (Non-linear Component to Model)\n",
        "            if verbose:\n",
        "                print(f\"   [2/4]  Calculating ARIMA residuals...\")\n",
        "            \n",
        "            # Get in-sample predictions and residuals\n",
        "            arima_train_pred = best_model.fittedvalues\n",
        "            train_residuals = train_data - arima_train_pred\n",
        "            \n",
        "            # Align arrays (ARIMA might drop initial values)\n",
        "            if len(train_residuals) < len(train_data):\n",
        "                n_dropped = len(train_data) - len(train_residuals)\n",
        "                train_data_aligned = train_data[n_dropped:]\n",
        "                train_residuals_aligned = train_residuals\n",
        "            else:\n",
        "                train_data_aligned = train_data\n",
        "                train_residuals_aligned = train_residuals\n",
        "            \n",
        "            if verbose:\n",
        "                print(f\"        Residuals: mean={np.mean(train_residuals_aligned):.6f}, std={np.std(train_residuals_aligned):.6f}\")\n",
        "            \n",
        "            # STEP 3: TRAIN SVM ON RESIDUALS\n",
        "\n",
        "            if len(train_residuals_aligned) <= lookback:\n",
        "                print(f\"     Insufficient data for SVM (need > {lookback} points)\")\n",
        "                continue\n",
        "            \n",
        "            if verbose:\n",
        "                print(f\"   [3/4]  Training SVM on residuals...\")\n",
        "            \n",
        "            # Create lagged features from residuals\n",
        "            X_train, y_train = create_lagged_features_from_residuals(train_residuals_aligned, lookback)\n",
        "            \n",
        "            if len(X_train) == 0:\n",
        "                print(f\"     No features created from residuals\")\n",
        "                continue\n",
        "            \n",
        "            # Scale features\n",
        "            X_train_scaled = scaler.fit_transform(X_train)\n",
        "            \n",
        "            # Build and train SVM\n",
        "            svm_model = build_residual_svm_model(kernel=kernel, C=C, epsilon=epsilon, gamma=gamma)\n",
        "            svm_model.fit(X_train_scaled, y_train)\n",
        "            \n",
        "            if verbose:\n",
        "                print(f\"       SVM trained with {len(X_train)} samples\")\n",
        "            \n",
        "            # STEP 4: HYBRID FORECASTING ON TEST SET\n",
        "\n",
        "\n",
        "            if verbose:\n",
        "                print(f\"   [4/4]  Generating hybrid forecasts...\")\n",
        "            \n",
        "            # Make ARIMA forecasts for test period\n",
        "            arima_test_pred = best_model.forecast(steps=len(test_data))\n",
        "            \n",
        "            # For SVM residual prediction, we need historical residuals\n",
        "            # Use last 'lookback' residuals from training + generate iteratively for test\n",
        "            \n",
        "            # Get last lookback residuals from training\n",
        "            last_residuals = train_residuals_aligned[-lookback:].flatten()\n",
        "            svm_residual_predictions = []\n",
        "            \n",
        "            # Predict residuals for each test point\n",
        "            for t in range(len(test_data)):\n",
        "                # Prepare input features (last lookback residuals)\n",
        "                X_input = last_residuals[-lookback:].reshape(1, -1)\n",
        "                X_input_scaled = scaler.transform(X_input)\n",
        "                \n",
        "                # Predict next residual\n",
        "                residual_pred = svm_model.predict(X_input_scaled)[0]\n",
        "                svm_residual_predictions.append(residual_pred)\n",
        "                \n",
        "                # Update sequence with actual residual for next iteration\n",
        "                if t < len(test_data) - 1:\n",
        "                    actual_residual = test_data[t] - arima_test_pred[t]\n",
        "                    last_residuals = np.append(last_residuals[1:], actual_residual)\n",
        "            \n",
        "            svm_residual_predictions = np.array(svm_residual_predictions)\n",
        "            \n",
        "            # HYBRID PREDICTION = ARIMA + SVM_residuals\n",
        "            hybrid_predictions = arima_test_pred + svm_residual_predictions\n",
        "            \n",
        "            # EVALUATE PERFORMANCE\n",
        "\n",
        "            # Metrics for ARIMA only\n",
        "            arima_rmse = np.sqrt(mean_squared_error(test_data, arima_test_pred))\n",
        "            arima_mae = mean_absolute_error(test_data, arima_test_pred)\n",
        "            \n",
        "            # Metrics for HYBRID\n",
        "            hybrid_rmse = np.sqrt(mean_squared_error(test_data, hybrid_predictions))\n",
        "            hybrid_mae = mean_absolute_error(test_data, hybrid_predictions)\n",
        "            \n",
        "            # Direction accuracy\n",
        "            arima_direction = np.mean(np.sign(test_data[1:]) == np.sign(arima_test_pred[1:])) * 100\n",
        "            hybrid_direction = np.mean(np.sign(test_data[1:]) == np.sign(hybrid_predictions[1:])) * 100\n",
        "            \n",
        "            improvement = ((arima_rmse - hybrid_rmse) / arima_rmse) * 100\n",
        "            \n",
        "            if verbose:\n",
        "                print(f\"    RESULTS:\")\n",
        "                print(f\"       ARIMA only:  RMSE={arima_rmse:.6f}, MAE={arima_mae:.6f}, Direction={arima_direction:.1f}%\")\n",
        "                print(f\"       HYBRID:      RMSE={hybrid_rmse:.6f}, MAE={hybrid_mae:.6f}, Direction={hybrid_direction:.1f}%\")\n",
        "                print(f\"       Improvement: {improvement:+.2f}% RMSE\")\n",
        "            \n",
        "            # Store results\n",
        "            window_results = {\n",
        "                'window_id': window_id,\n",
        "                'train_start': split['train']['start'],\n",
        "                'train_end': split['train']['end'],\n",
        "                'test_start': split['test']['start'],\n",
        "                'test_end': split['test']['end'],\n",
        "                'arima_order': best_order,\n",
        "                'arima_aic': best_aic,\n",
        "                # ARIMA metrics\n",
        "                'arima_rmse': arima_rmse,\n",
        "                'arima_mae': arima_mae,\n",
        "                'arima_direction_accuracy': arima_direction,\n",
        "                'arima_predictions': arima_test_pred,\n",
        "                # SVM residual metrics\n",
        "                'svm_residual_predictions': svm_residual_predictions,\n",
        "                # HYBRID metrics\n",
        "                'hybrid_rmse': hybrid_rmse,\n",
        "                'hybrid_mae': hybrid_mae,\n",
        "                'hybrid_direction_accuracy': hybrid_direction,\n",
        "                'hybrid_predictions': hybrid_predictions,\n",
        "                # Comparison\n",
        "                'rmse_improvement_pct': improvement,\n",
        "                'actuals': test_data\n",
        "            }\n",
        "            \n",
        "            all_results.append(window_results)\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"   Error in window {window_id}: {str(e)}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            continue\n",
        "    \n",
        "    # SUMMARY \n",
        "    if len(all_results) > 0:\n",
        "        avg_arima_rmse = np.mean([r['arima_rmse'] for r in all_results])\n",
        "        avg_hybrid_rmse = np.mean([r['hybrid_rmse'] for r in all_results])\n",
        "        avg_improvement = np.mean([r['rmse_improvement_pct'] for r in all_results])\n",
        "\n",
        "        avg_arima_mae = np.mean([r['arima_mae'] for r in all_results])\n",
        "        avg_hybrid_mae = np.mean([r['hybrid_mae'] for r in all_results])\n",
        "        \n",
        "        avg_arima_direction = np.mean([r['arima_direction_accuracy'] for r in all_results])\n",
        "        avg_hybrid_direction = np.mean([r['hybrid_direction_accuracy'] for r in all_results])\n",
        "        \n",
        "        print(f\"\\n{'='*100}\")\n",
        "        print(f\" {asset_name.upper()} HYBRID MODEL COMPLETE\")\n",
        "        print(f\"{'='*100}\")\n",
        "        print(f\"Windows processed: {len(all_results)}/{len(cv_splits)}\")\n",
        "        print(f\"\\n AVERAGE PERFORMANCE:\")\n",
        "        print(f\"   ARIMA only:  RMSE={avg_arima_rmse:.6f}, MAE={avg_arima_mae:.6f}, Direction={avg_arima_direction:.2f}%\")\n",
        "        print(f\"   HYBRID:      RMSE={avg_hybrid_rmse:.6f}, MAE={avg_hybrid_mae:.6f}, Direction={avg_hybrid_direction:.2f}%\")\n",
        "        print(f\"   Improvement: {avg_improvement:+.2f}% RMSE\")\n",
        "        print(f\"{'='*100}\")\n",
        "        \n",
        "        results_dict = {\n",
        "            'asset_name': asset_name,\n",
        "            'model_type': 'HYBRID_ARIMA_SVM',\n",
        "            'windows_processed': len(all_results),\n",
        "            'total_windows': len(cv_splits),\n",
        "            # ARIMA metrics\n",
        "            'avg_arima_rmse': avg_arima_rmse,\n",
        "            'avg_arima_direction': avg_arima_direction,\n",
        "            'avg_arima_mae': avg_arima_mae,\n",
        "            # HYBRID metrics\n",
        "            'avg_hybrid_rmse': avg_hybrid_rmse,\n",
        "            'avg_hybrid_direction': avg_hybrid_direction,\n",
        "            'avg_hybrid_mae': avg_hybrid_mae,\n",
        "            # Improvement\n",
        "            'avg_improvement_pct': avg_improvement,\n",
        "            'window_results': all_results,\n",
        "            'hyperparameters': {\n",
        "                'arima': f'p∈[0,{max_p}], d∈[0,{max_d}], q∈[0,{max_q}]',\n",
        "                'svm_lookback': lookback,\n",
        "                'svm_kernel': kernel,\n",
        "                'svm_C': C,\n",
        "                'svm_epsilon': epsilon,\n",
        "                'svm_gamma': gamma\n",
        "            }\n",
        "        }\n",
        "        \n",
        "        return results_dict\n",
        "    else:\n",
        "        print(f\"\\n No windows successfully processed for {asset_name}\")\n",
        "        return None\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0d6a43b",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n S&P 500 HYBRID ARIMA-SVM\")\n",
        "\n",
        "sp500_hybrid_results = run_hybrid_arima_svm_cv(\n",
        "    cv_splits=sp500_cv_splits,\n",
        "    data_clean=sp500_clean,\n",
        "    asset_name='S&P 500',\n",
        "    max_p=3,\n",
        "    max_d=1,\n",
        "    max_q=3,\n",
        "    lookback=10,\n",
        "    kernel='rbf',\n",
        "    C=1.0,\n",
        "    epsilon=0.1,\n",
        "    gamma='scale',\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "print(\"\\n\\n BITCOIN HYBRID ARIMA-SVM\")\n",
        "\n",
        "bitcoin_hybrid_results = run_hybrid_arima_svm_cv(\n",
        "    cv_splits=bitcoin_cv_splits,\n",
        "    data_clean=bitcoin_clean,\n",
        "    asset_name='Bitcoin',\n",
        "    max_p=3,\n",
        "    max_d=1,\n",
        "    max_q=3,\n",
        "    lookback=10,\n",
        "    kernel='rbf',\n",
        "    C=1.0,\n",
        "    epsilon=0.1,\n",
        "    gamma='scale',\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "print(\" FINAL \")\n",
        "\n",
        "\n",
        "summary_data = []\n",
        "\n",
        "if sp500_hybrid_results:\n",
        "    summary_data.append({\n",
        "        'Asset': 'S&P 500',\n",
        "        'ARIMA_RMSE': sp500_hybrid_results['avg_arima_rmse'],\n",
        "        'Hybrid_RMSE': sp500_hybrid_results['avg_hybrid_rmse'],\n",
        "        'Improvement_%': sp500_hybrid_results['avg_improvement_pct'],\n",
        "        'ARIMA_MAE': sp500_hybrid_results['avg_arima_mae'],\n",
        "        'Hybrid_MAE': sp500_hybrid_results['avg_hybrid_mae'],\n",
        "        'ARIMA_Direction_%': sp500_hybrid_results['avg_arima_direction'],\n",
        "        'Hybrid_Direction_%': sp500_hybrid_results['avg_hybrid_direction'],\n",
        "        'Windows': f\"{sp500_hybrid_results['windows_processed']}/{sp500_hybrid_results['total_windows']}\"\n",
        "    })\n",
        "\n",
        "if bitcoin_hybrid_results:\n",
        "    summary_data.append({\n",
        "        'Asset': 'Bitcoin',\n",
        "        'ARIMA_RMSE': bitcoin_hybrid_results['avg_arima_rmse'],\n",
        "        'Hybrid_RMSE': bitcoin_hybrid_results['avg_hybrid_rmse'],\n",
        "        'Improvement_%': bitcoin_hybrid_results['avg_improvement_pct'],\n",
        "        'ARIMA_MAE': bitcoin_hybrid_results['avg_arima_mae'],\n",
        "        'Hybrid_MAE': bitcoin_hybrid_results['avg_hybrid_mae'],\n",
        "        'ARIMA_Direction_%': bitcoin_hybrid_results['avg_arima_direction'],\n",
        "        'Hybrid_Direction_%': bitcoin_hybrid_results['avg_hybrid_direction'],\n",
        "        'Windows': f\"{bitcoin_hybrid_results['windows_processed']}/{bitcoin_hybrid_results['total_windows']}\"\n",
        "    })\n",
        "\n",
        "if summary_data:\n",
        "    summary_df = pd.DataFrame(summary_data)\n",
        "    print(\"\\n\", summary_df.to_string(index=False))\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 100)\n",
        "    print(\" KEY INSIGHTS:\")\n",
        "    print(\"=\" * 100)\n",
        "    \n",
        "    for data in summary_data:\n",
        "        asset = data['Asset']\n",
        "        improvement = data['Improvement_%']\n",
        "        \n",
        "        if improvement > 0:\n",
        "            print(f\"✓ {asset}: Hybrid model OUTPERFORMS ARIMA by {improvement:.2f}% RMSE\")\n",
        "        elif improvement < 0:\n",
        "            print(f\"✗ {asset}: Hybrid model underperforms ARIMA by {abs(improvement):.2f}% RMSE\")\n",
        "        else:\n",
        "            print(f\"= {asset}: Hybrid model equals ARIMA performance\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1b5258c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualization Functions for Hybrid ARIMA-SVM Results\n",
        "\n",
        "def plot_hybrid_decomposition(results, window_idx=0):\n",
        "    if not results or len(results['window_results']) == 0:\n",
        "        print(\"No results to plot\")\n",
        "        return\n",
        "    \n",
        "    window = results['window_results'][window_idx]\n",
        "    asset_name = results['asset_name']\n",
        "    \n",
        "    # Extract data\n",
        "    actuals = window['actuals']\n",
        "    arima_pred = window['arima_predictions']\n",
        "    svm_residual = window['svm_residual_predictions']\n",
        "    hybrid_pred = window['hybrid_predictions']\n",
        "    \n",
        "    # Create figure with subplots\n",
        "    fig, axes = plt.subplots(3, 2, figsize=(16, 12))\n",
        "    fig.suptitle(f'{asset_name} - Hybrid ARIMA-SVM Decomposition (Window {window[\"window_id\"]})', \n",
        "                 fontsize=16, fontweight='bold')\n",
        "    \n",
        "    # Plot 1: Actual vs ARIMA\n",
        "    ax1 = axes[0, 0]\n",
        "    ax1.plot(actuals, label='Actual', color='black', linewidth=2, alpha=0.7)\n",
        "    ax1.plot(arima_pred, label='ARIMA', color='blue', linewidth=1.5, linestyle='--', alpha=0.7)\n",
        "    ax1.set_title('ARIMA Component (Linear)', fontsize=12, fontweight='bold')\n",
        "    ax1.set_ylabel('Log Returns')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    ax1.text(0.02, 0.98, f'RMSE: {window[\"arima_rmse\"]:.6f}', \n",
        "             transform=ax1.transAxes, verticalalignment='top',\n",
        "             bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
        "    \n",
        "    # Plot 2: ARIMA Residuals\n",
        "    ax2 = axes[0, 1]\n",
        "    arima_residuals = actuals - arima_pred\n",
        "    ax2.plot(arima_residuals, color='red', linewidth=1, alpha=0.7)\n",
        "    ax2.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
        "    ax2.fill_between(range(len(arima_residuals)), arima_residuals, 0, alpha=0.3, color='red')\n",
        "    ax2.set_title('ARIMA Residuals (Non-linear to Model)', fontsize=12, fontweight='bold')\n",
        "    ax2.set_ylabel('Residuals')\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    ax2.text(0.02, 0.98, f'Std: {np.std(arima_residuals):.6f}', \n",
        "             transform=ax2.transAxes, verticalalignment='top',\n",
        "             bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
        "    \n",
        "    # Plot 3: SVM Residual Predictions\n",
        "    ax3 = axes[1, 0]\n",
        "    ax3.plot(arima_residuals, label='Actual Residuals', color='red', linewidth=1.5, alpha=0.7)\n",
        "    ax3.plot(svm_residual, label='SVM Predicted', color='green', linewidth=1.5, linestyle='--', alpha=0.7)\n",
        "    ax3.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
        "    ax3.set_title('SVM Residual Predictions', fontsize=12, fontweight='bold')\n",
        "    ax3.set_ylabel('Residuals')\n",
        "    ax3.legend()\n",
        "    ax3.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Plot 4: Actual vs Hybrid\n",
        "    ax4 = axes[1, 1]\n",
        "    ax4.plot(actuals, label='Actual', color='black', linewidth=2, alpha=0.7)\n",
        "    ax4.plot(hybrid_pred, label='Hybrid (ARIMA+SVM)', color='purple', linewidth=1.5, linestyle='--', alpha=0.7)\n",
        "    ax4.set_title('Final Hybrid Forecast', fontsize=12, fontweight='bold')\n",
        "    ax4.set_ylabel('Log Returns')\n",
        "    ax4.legend()\n",
        "    ax4.grid(True, alpha=0.3)\n",
        "    ax4.text(0.02, 0.98, f'RMSE: {window[\"hybrid_rmse\"]:.6f}', \n",
        "             transform=ax4.transAxes, verticalalignment='top',\n",
        "             bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
        "    \n",
        "    # Plot 5: Component Comparison\n",
        "    ax5 = axes[2, 0]\n",
        "    sample_size = min(50, len(actuals))\n",
        "    x = range(sample_size)\n",
        "    ax5.plot(x, actuals[:sample_size], label='Actual', color='black', linewidth=2, alpha=0.7)\n",
        "    ax5.plot(x, arima_pred[:sample_size], label='ARIMA', color='blue', linewidth=1.5, linestyle='--', alpha=0.7)\n",
        "    ax5.plot(x, hybrid_pred[:sample_size], label='Hybrid', color='purple', linewidth=1.5, linestyle='--', alpha=0.7)\n",
        "    ax5.set_title(f'First {sample_size} Predictions Comparison', fontsize=12, fontweight='bold')\n",
        "    ax5.set_xlabel('Time Step')\n",
        "    ax5.set_ylabel('Log Returns')\n",
        "    ax5.legend()\n",
        "    ax5.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Plot 6: Performance Metrics\n",
        "    ax6 = axes[2, 1]\n",
        "    metrics = ['RMSE', 'MAE', 'Direction Acc (%)']\n",
        "    arima_metrics = [window['arima_rmse'], window['arima_mae'], window['arima_direction_accuracy']]\n",
        "    hybrid_metrics = [window['hybrid_rmse'], window['hybrid_mae'], window['hybrid_direction_accuracy']]\n",
        "    \n",
        "    x_pos = np.arange(len(metrics))\n",
        "    width = 0.35\n",
        "    \n",
        "    bars1 = ax6.bar(x_pos - width/2, arima_metrics, width, label='ARIMA', alpha=0.8)\n",
        "    bars2 = ax6.bar(x_pos + width/2, hybrid_metrics, width, label='Hybrid', alpha=0.8)\n",
        "    \n",
        "    ax6.set_ylabel('Value')\n",
        "    ax6.set_title('Performance Metrics Comparison', fontsize=12, fontweight='bold')\n",
        "    ax6.set_xticks(x_pos)\n",
        "    ax6.set_xticklabels(metrics)\n",
        "    ax6.legend()\n",
        "    ax6.grid(True, alpha=0.3, axis='y')\n",
        "    \n",
        "    # Add value labels\n",
        "    for bars in [bars1, bars2]:\n",
        "        for bar in bars:\n",
        "            height = bar.get_height()\n",
        "            ax6.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                    f'{height:.4f}', ha='center', va='bottom', fontsize=8)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Print detailed stats\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"DETAILED STATISTICS - Window {window['window_id']}\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"Period: {window['test_start'].strftime('%Y-%m-%d')} to {window['test_end'].strftime('%Y-%m-%d')}\")\n",
        "    print(f\"ARIMA Order: {window['arima_order']}, AIC: {window['arima_aic']:.2f}\")\n",
        "    print(f\"\\nARIMA Performance:\")\n",
        "    print(f\"  • RMSE: {window['arima_rmse']:.6f}\")\n",
        "    print(f\"  • MAE: {window['arima_mae']:.6f}\")\n",
        "    print(f\"  • Direction Accuracy: {window['arima_direction_accuracy']:.2f}%\")\n",
        "    print(f\"\\nHybrid Performance:\")\n",
        "    print(f\"  • RMSE: {window['hybrid_rmse']:.6f}\")\n",
        "    print(f\"  • MAE: {window['hybrid_mae']:.6f}\")\n",
        "    print(f\"  • Direction Accuracy: {window['hybrid_direction_accuracy']:.2f}%\")\n",
        "    print(f\"\\nImprovement: {window['rmse_improvement_pct']:+.2f}% RMSE\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "\n",
        "def plot_all_windows_comparison(results):\n",
        "    if not results or len(results['window_results']) == 0:\n",
        "        print(\"No results to plot\")\n",
        "        return\n",
        "    \n",
        "    asset_name = results['asset_name']\n",
        "    windows = [r['window_id'] for r in results['window_results']]\n",
        "    arima_rmse = [r['arima_rmse'] for r in results['window_results']]\n",
        "    hybrid_rmse = [r['hybrid_rmse'] for r in results['window_results']]\n",
        "    improvement = [r['rmse_improvement_pct'] for r in results['window_results']]\n",
        "    \n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "    fig.suptitle(f'{asset_name} - Hybrid Model Performance Across All Windows', \n",
        "                 fontsize=14, fontweight='bold')\n",
        "    \n",
        "    # Plot 1: RMSE Comparison\n",
        "    ax1 = axes[0]\n",
        "    ax1.plot(windows, arima_rmse, marker='o', label='ARIMA', linewidth=2, markersize=6)\n",
        "    ax1.plot(windows, hybrid_rmse, marker='s', label='Hybrid', linewidth=2, markersize=6)\n",
        "    ax1.set_xlabel('Window ID')\n",
        "    ax1.set_ylabel('RMSE')\n",
        "    ax1.set_title('RMSE by Window', fontsize=12, fontweight='bold')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Plot 2: Improvement\n",
        "    ax2 = axes[1]\n",
        "    colors = ['green' if x > 0 else 'red' for x in improvement]\n",
        "    ax2.bar(windows, improvement, color=colors, alpha=0.7)\n",
        "    ax2.axhline(y=0, color='black', linestyle='-', linewidth=1)\n",
        "    ax2.set_xlabel('Window ID')\n",
        "    ax2.set_ylabel('RMSE Improvement (%)')\n",
        "    ax2.set_title('Hybrid Improvement over ARIMA', fontsize=12, fontweight='bold')\n",
        "    ax2.grid(True, alpha=0.3, axis='y')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Summary statistics\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"{asset_name.upper()} - SUMMARY STATISTICS\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"Total Windows: {len(windows)}\")\n",
        "    print(f\"Windows where Hybrid outperforms: {sum(1 for x in improvement if x > 0)}\")\n",
        "    print(f\"Windows where ARIMA outperforms: {sum(1 for x in improvement if x < 0)}\")\n",
        "    print(f\"Average Improvement: {np.mean(improvement):.2f}%\")\n",
        "    print(f\"Best Improvement: {max(improvement):.2f}% (Window {windows[improvement.index(max(improvement))]})\")\n",
        "    print(f\"Worst Improvement: {min(improvement):.2f}% (Window {windows[improvement.index(min(improvement))]})\")\n",
        "    print(f\"{'='*80}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a473dbd8",
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_all_windows_comparison(sp500_hybrid_results)\n",
        "plot_all_windows_comparison(bitcoin_hybrid_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8345f710",
      "metadata": {},
      "source": [
        "# EURUSD Dataset Analysis\n",
        "\n",
        "## ARIMA + SVM Hybrid\n",
        "Dataset: EUR/USD Exchange Rate (2009-08-11 to 2019-08-11)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "261cec05",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download EURUSD data\n",
        "print(\"Downloading EURUSD data...\")\n",
        "eurusd_data = yf.download(\"EURUSD=X\", start=\"2009-08-11\", end=\"2019-08-11\", progress=False)\n",
        "\n",
        "print(f\"\\nEURUSD Data Shape: {eurusd_data.shape}\")\n",
        "print(f\"EURUSD Date Range: {eurusd_data.index.min()} to {eurusd_data.index.max()}\")\n",
        "print(f\"Total EURUSD observations: {len(eurusd_data)}\")\n",
        "\n",
        "# Calculate log returns\n",
        "eurusd_data['Log_Returns'] = np.log(eurusd_data['Close'] / eurusd_data['Close'].shift(1))\n",
        "\n",
        "# Clean data\n",
        "eurusd_clean = eurusd_data.dropna()\n",
        "\n",
        "print(f\"\\nAfter cleaning: {len(eurusd_clean)} observations\")\n",
        "print(f\"Mean daily return: {eurusd_clean['Log_Returns'].mean():.6f}\")\n",
        "print(f\"Standard deviation: {eurusd_clean['Log_Returns'].std():.6f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90f27ee6",
      "metadata": {},
      "outputs": [],
      "source": [
        "from dateutil.relativedelta import relativedelta\n",
        "from datetime import timedelta\n",
        "\n",
        "def create_eurusd_cv_splits(data, start_date=None):\n",
        "    \n",
        "    if start_date is None:\n",
        "        start_date = data.index.min()\n",
        "    \n",
        "    cv_splits = []\n",
        "    window_start = start_date\n",
        "    \n",
        "    while True:\n",
        "        # Define window boundaries\n",
        "        train_start = window_start\n",
        "        train_end = train_start + relativedelta(years=2) - timedelta(days=1)\n",
        "        \n",
        "        # Validation periods (8, 16, 24 months)\n",
        "        val_start = train_end + timedelta(days=1)\n",
        "        val1_end = val_start + relativedelta(months=8) - timedelta(days=1)\n",
        "        val2_end = val_start + relativedelta(months=16) - timedelta(days=1)\n",
        "        val3_end = val_start + relativedelta(months=24) - timedelta(days=1)\n",
        "        \n",
        "        # Test period (6 months)\n",
        "        test_start = val3_end + timedelta(days=1)\n",
        "        test_end = test_start + relativedelta(months=6) - timedelta(days=1)\n",
        "        \n",
        "        # Check if we have enough data\n",
        "        if test_end > data.index.max():\n",
        "            break\n",
        "            \n",
        "        # Create splits for this window\n",
        "        train_data = data[(data.index >= train_start) & (data.index <= train_end)]\n",
        "        \n",
        "        # Three validation folds\n",
        "        val1_data = data[(data.index >= val_start) & (data.index <= val1_end)]\n",
        "        val2_data = data[(data.index >= val_start) & (data.index <= val2_end)]\n",
        "        val3_data = data[(data.index >= val_start) & (data.index <= val3_end)]\n",
        "        \n",
        "        test_data = data[(data.index >= test_start) & (data.index <= test_end)]\n",
        "        \n",
        "        cv_splits.append({\n",
        "            'window_id': len(cv_splits) + 1,\n",
        "            'train': {\n",
        "                'data': train_data,\n",
        "                'start': train_start,\n",
        "                'end': train_end,\n",
        "                'size': len(train_data)\n",
        "            },\n",
        "            'validation': [\n",
        "                {\n",
        "                    'fold': 1,\n",
        "                    'data': val1_data,\n",
        "                    'start': val_start,\n",
        "                    'end': val1_end,\n",
        "                    'size': len(val1_data),\n",
        "                    'months': 8\n",
        "                },\n",
        "                {\n",
        "                    'fold': 2,\n",
        "                    'data': val2_data,\n",
        "                    'start': val_start,\n",
        "                    'end': val2_end,\n",
        "                    'size': len(val2_data),\n",
        "                    'months': 16\n",
        "                },\n",
        "                {\n",
        "                    'fold': 3,\n",
        "                    'data': val3_data,\n",
        "                    'start': val_start,\n",
        "                    'end': val3_end,\n",
        "                    'size': len(val3_data),\n",
        "                    'months': 24\n",
        "                }\n",
        "            ],\n",
        "            'test': {\n",
        "                'data': test_data,\n",
        "                'start': test_start,\n",
        "                'end': test_end,\n",
        "                'size': len(test_data)\n",
        "            }\n",
        "        })\n",
        "        \n",
        "        # Move window forward by 1 year\n",
        "        window_start += relativedelta(years=1)\n",
        "    \n",
        "    return cv_splits\n",
        "\n",
        "eurusd_cv_splits = create_eurusd_cv_splits(eurusd_clean)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7b8f140",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\\n New Dataset: EURUSD HYBRID ARIMA-SVM\")\n",
        "\n",
        "eurusd_hybrid_results = run_hybrid_arima_svm_cv(\n",
        "    cv_splits=eurusd_cv_splits,\n",
        "    data_clean=eurusd_clean,\n",
        "    asset_name='EURUSD',\n",
        "    max_p=3,\n",
        "    max_d=1,\n",
        "    max_q=3,\n",
        "    lookback=10,\n",
        "    kernel='rbf',\n",
        "    C=1.0,\n",
        "    epsilon=0.1,\n",
        "    gamma='scale',\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fdde0ae2",
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_all_windows_comparison(eurusd_hybrid_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56b9abab",
      "metadata": {},
      "source": [
        "# Trading Performance Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "751f2aba",
      "metadata": {},
      "outputs": [],
      "source": [
        "def volatility_predictions_to_returns_new(predictions, true_values, actual_returns, transaction_costs=0.0):\n",
        "\n",
        "    # Ensure all arrays have matching length\n",
        "    min_len = min(len(predictions), len(true_values), len(actual_returns))\n",
        "    predictions = predictions[:min_len]\n",
        "    true_values = true_values[:min_len]\n",
        "    actual_returns = (actual_returns.iloc[:min_len] \n",
        "                     if isinstance(actual_returns, pd.Series) \n",
        "                     else actual_returns[:min_len])\n",
        "    \n",
        "    # Convert to numpy arrays for consistency\n",
        "    actual_returns_array = (actual_returns.values \n",
        "                           if isinstance(actual_returns, pd.Series) \n",
        "                           else actual_returns)\n",
        "\n",
        "    signal = np.where(predictions > transaction_costs, 1, -1)\n",
        "\n",
        "    if transaction_costs > 0.0:\n",
        "        signals = np.where(np.abs(actual_returns_array) > transaction_costs, signal, 0)\n",
        "    \n",
        "\n",
        "    strategy_returns = signals * actual_returns_array\n",
        "    \n",
        "    return pd.Series(strategy_returns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df2a39bd",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_all_predictions(model_results, data_clean, model_type=\"S&P\", window_indices=None):\n",
        "    if model_type == \"S&P\":\n",
        "        cost = 0.005\n",
        "    elif model_type == \"Bitcoin\":\n",
        "        cost = 0.01\n",
        "    else:\n",
        "        cost = 0.001\n",
        "    all_strategy_returns = []\n",
        "\n",
        "    windows_to_use = model_results['window_results']\n",
        "    if window_indices is not None:\n",
        "        windows_to_use = [w for w in windows_to_use if w['window_id'] in window_indices]\n",
        "    \n",
        "    for window_result in windows_to_use:\n",
        "        try:\n",
        "            test_start = window_result['test_start']\n",
        "            test_end = window_result['test_end']\n",
        "            test_data = data_clean[test_start:test_end]\n",
        "            \n",
        "            predictions = window_result['hybrid_predictions']\n",
        "            true_values = test_data['Log_Returns'].values[-len(predictions):]\n",
        "            \n",
        "            actual_returns = test_data['Log_Returns'].iloc[-len(predictions):]\n",
        "            \n",
        "            window_returns = volatility_predictions_to_returns_new(\n",
        "                predictions, true_values, actual_returns.values, transaction_costs=cost\n",
        "            )\n",
        "            \n",
        "            all_strategy_returns.append(window_returns)\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Failed to process window {window_result.get('window_id', '?')}: {str(e)}\")\n",
        "            continue\n",
        "    \n",
        "    if all_strategy_returns:\n",
        "        return pd.concat(all_strategy_returns, ignore_index=True)\n",
        "    else:\n",
        "        return pd.Series([])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24404560",
      "metadata": {},
      "outputs": [],
      "source": [
        "def annualized_return(daily_returns):\n",
        "    cumulative = (1 + daily_returns).prod()\n",
        "    n = daily_returns.shape[0]\n",
        "    return cumulative ** (TRADING_DAYS / n) - 1\n",
        "\n",
        "\n",
        "def annualized_std(daily_returns):\n",
        "    return daily_returns.std() * np.sqrt(TRADING_DAYS)\n",
        "\n",
        "\n",
        "def max_drawdown(daily_returns):\n",
        "    equity = (1 + daily_returns).cumprod()\n",
        "    peak = equity.cummax()\n",
        "    drawdown = (equity - peak) / peak\n",
        "    return np.abs(drawdown.min())  # Paper uses absolute value\n",
        "\n",
        "\n",
        "def information_ratio(strategy_returns, benchmark_returns):\n",
        "    arc = annualized_return(strategy_returns)\n",
        "    asd = annualized_std(strategy_returns)\n",
        "    \n",
        "    if asd == 0:\n",
        "        return np.nan\n",
        "    return arc / asd\n",
        "\n",
        "\n",
        "def modified_information_ratio(strategy_returns, benchmark_returns):\n",
        "    arc = annualized_return(strategy_returns)\n",
        "    asd = annualized_std(strategy_returns)\n",
        "    md = max_drawdown(strategy_returns)\n",
        "    \n",
        "    if asd == 0 or md == 0:\n",
        "        return np.nan\n",
        "        \n",
        "    return (arc * np.sign(arc) * arc) / (asd * md)\n",
        "\n",
        "\n",
        "def sortino_ratio(daily_returns, risk_free_rate=0):\n",
        "    negative_returns = daily_returns[daily_returns < 0]\n",
        "    \n",
        "    if len(negative_returns) == 0:\n",
        "        return np.nan\n",
        "        \n",
        "    downside_std = np.std(negative_returns, ddof=1)\n",
        "    asd_downside = downside_std * np.sqrt(TRADING_DAYS)\n",
        "    \n",
        "    arc = annualized_return(daily_returns)\n",
        "    \n",
        "    if asd_downside == 0:\n",
        "        return np.nan\n",
        "        \n",
        "    return arc / asd_downside\n",
        "\n",
        "\n",
        "def compute_performance_indicators(strategy_returns, benchmark_returns):\n",
        "    return {\n",
        "        \"ARC\": annualized_return(strategy_returns),\n",
        "        \"ASD\": annualized_std(strategy_returns),\n",
        "        \"MD\": abs(max_drawdown(strategy_returns)),\n",
        "        \"IR\": information_ratio(strategy_returns, benchmark_returns),\n",
        "        \"IR*\": modified_information_ratio(strategy_returns, benchmark_returns),\n",
        "        \"SR\": sortino_ratio(strategy_returns)\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b13e6eb",
      "metadata": {},
      "source": [
        "## S&P 500"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1766534",
      "metadata": {},
      "outputs": [],
      "source": [
        "TRADING_DAYS = 232\n",
        "sp500_bnh_returns = sp500_clean['Log_Returns'].loc[\"2007-01-01\":\"2023-12-29\"].values\n",
        "\n",
        "sp500_hybrid_predictions = get_all_predictions(sp500_hybrid_results, sp500_clean)\n",
        "\n",
        "sp500_hybrid_strategy_returns = sp500_hybrid_predictions\n",
        "\n",
        "sp500_bnh_aligned = sp500_bnh_returns[-len(sp500_hybrid_strategy_returns):]\n",
        "\n",
        "results_sp500 = []\n",
        "\n",
        "# HYBRID\n",
        "hybrid_metrics = compute_performance_indicators(\n",
        "    pd.Series(sp500_hybrid_strategy_returns),\n",
        "    pd.Series(sp500_bnh_aligned)\n",
        ")\n",
        "hybrid_metrics['Model'] = 'HYBRID'\n",
        "hybrid_metrics['Num_Trades'] = int(np.sum(np.abs(np.diff(sp500_hybrid_strategy_returns > 0)) > 0))\n",
        "results_sp500.append(hybrid_metrics)\n",
        "\n",
        "\n",
        "table2_sp500 = pd.DataFrame(results_sp500)\n",
        "\n",
        "\n",
        "print(\"TABLE: S&P 500 Long-Short Strategy Results\")\n",
        "\n",
        "print(table2_sp500[['Model', 'ARC', 'ASD', 'MD', 'IR', 'IR*', 'SR']].to_string(index=False))\n",
        "\n",
        "table2_sp500.to_csv('table2_sp500.csv', index=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41df17b4",
      "metadata": {},
      "source": [
        "## Bitcoin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf386f21",
      "metadata": {},
      "outputs": [],
      "source": [
        "TRADING_DAYS = 345\n",
        "\n",
        "# Get benchmark returns (Buy-and-Hold)\n",
        "bitcoin_bnh_returns = bitcoin_clean['Log_Returns'].values\n",
        "\n",
        "bitcoin_hybrid_predictions = get_all_predictions(bitcoin_hybrid_results, bitcoin_clean, model_type=\"bitcoin\")\n",
        "\n",
        "bitcoin_hybrid_strategy_returns = bitcoin_hybrid_predictions\n",
        "    \n",
        "bitcoin_bnh_aligned = bitcoin_bnh_returns[-len(bitcoin_hybrid_strategy_returns):]\n",
        "\n",
        "\n",
        "results_bitcoin = []\n",
        "\n",
        "# HYBRID\n",
        "hybrid_metrics = compute_performance_indicators(\n",
        "    pd.Series(bitcoin_hybrid_strategy_returns),\n",
        "    pd.Series(bitcoin_bnh_aligned)\n",
        ")\n",
        "hybrid_metrics['Model'] = 'HYBRID'\n",
        "hybrid_metrics['Num_Trades'] = int(np.sum(np.abs(np.diff(bitcoin_hybrid_strategy_returns > 0)) > 0))\n",
        "results_bitcoin.append(hybrid_metrics)\n",
        "\n",
        "table2_bitcoin = pd.DataFrame(results_bitcoin)\n",
        "\n",
        "print(\"TABLE: Bitcoin Long-Short Strategy Results\")\n",
        "\n",
        "print(table2_bitcoin[['Model', 'ARC', 'ASD', 'MD', 'IR', 'IR*', 'SR']].to_string(index=False))\n",
        "\n",
        "table2_bitcoin.to_csv('table2_bitcoin.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f0759fc",
      "metadata": {},
      "source": [
        "## EURUSD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb710695",
      "metadata": {},
      "outputs": [],
      "source": [
        "TRADING_DAYS = 252\n",
        "\n",
        "# Get benchmark returns (Buy-and-Hold)\n",
        "eurusd_bnh_returns = eurusd_clean['Log_Returns'].values\n",
        "\n",
        "\n",
        "\n",
        "eurusd_hybrid_predictions = get_all_predictions(eurusd_hybrid_results, eurusd_clean,model_type=\"EURUSD\")\n",
        "\n",
        "eurusd_hybrid_strategy_returns = eurusd_hybrid_predictions\n",
        "\n",
        "eurusd_bnh_aligned = eurusd_bnh_returns[-len(eurusd_hybrid_strategy_returns):]\n",
        "\n",
        "\n",
        "results_eurusd = []\n",
        "\n",
        "# HYBRID\n",
        "hybrid_metrics = compute_performance_indicators(\n",
        "    pd.Series(eurusd_hybrid_strategy_returns),\n",
        "    pd.Series(eurusd_bnh_aligned)\n",
        ")\n",
        "hybrid_metrics['Model'] = 'HYBRID'\n",
        "hybrid_metrics['Num_Trades'] = int(np.sum(np.abs(np.diff(eurusd_hybrid_strategy_returns > 0)) > 0))\n",
        "results_eurusd.append(hybrid_metrics)\n",
        "\n",
        "table2_eurusd = pd.DataFrame(results_eurusd)\n",
        "\n",
        "\n",
        "print(\"TABLE: EURUSD Long-Short Strategy Results\")\n",
        "\n",
        "print(table2_eurusd[['Model', 'ARC', 'ASD', 'MD', 'IR', 'IR*', 'SR']].to_string(index=False))\n",
        "\n",
        "table2_eurusd.to_csv('table2_eurusd.csv', index=False)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
