{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM\n",
    "from arch import arch_model\n",
    "import warnings\n",
    "import random\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "random.seed(456)\n",
    "np.random.seed(456)\n",
    "tf.random.set_seed(456)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Download Configuration\n",
    "# S&P 500: January 1, 2002 to December 31, 2023\n",
    "# Bitcoin: January 1, 2015 to December 31, 2023\n",
    "\n",
    "# Define date ranges\n",
    "sp500_start = \"2002-01-01\"\n",
    "sp500_end = \"2023-12-31\"\n",
    "bitcoin_start = \"2015-01-01\"\n",
    "bitcoin_end = \"2023-12-31\"\n",
    "\n",
    "print(\"Downloading S&P 500 data...\")\n",
    "sp500_data = yf.download(\"^GSPC\", start=sp500_start, end=sp500_end, progress=False)\n",
    "\n",
    "print(\"Downloading Bitcoin data...\")\n",
    "bitcoin_data = yf.download(\"BTC-USD\", start=bitcoin_start, end=bitcoin_end, progress=False)\n",
    "\n",
    "print(\"Downloading EURUSD data...\")\n",
    "# Download EURUSD data\n",
    "eurusd_data = yf.download(\"EURUSD=X\", start=\"2009-08-11\", end=\"2019-08-11\", progress=False)\n",
    "\n",
    "# Display basic information about downloaded data\n",
    "print(f\"\\nS&P 500 Data Shape: {sp500_data.shape}\")\n",
    "print(f\"S&P 500 Date Range: {sp500_data.index.min()} to {sp500_data.index.max()}\")\n",
    "print(f\"Total S&P 500 observations: {len(sp500_data)}\")\n",
    "\n",
    "print(f\"\\nBitcoin Data Shape: {bitcoin_data.shape}\")\n",
    "print(f\"Bitcoin Date Range: {bitcoin_data.index.min()} to {bitcoin_data.index.max()}\")\n",
    "print(f\"Total Bitcoin observations: {len(bitcoin_data)}\")\n",
    "\n",
    "print(f\"\\nEURUSD Data Shape: {eurusd_data.shape}\")\n",
    "print(f\"EURUSD Date Range: {eurusd_data.index.min()} to {eurusd_data.index.max()}\")\n",
    "print(f\"Total EURUSD observations: {len(eurusd_data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate log returns\n",
    "sp500_data['Log_Returns'] = np.log(sp500_data['Close'] / sp500_data['Close'].shift(1))\n",
    "bitcoin_data['Log_Returns'] = np.log(bitcoin_data['Close'] / bitcoin_data['Close'].shift(1))\n",
    "eurusd_data['Log_Returns'] = np.log(eurusd_data['Close'] / eurusd_data['Close'].shift(1))\n",
    "\n",
    "# Calculate realized volatility (squared returns as proxy)\n",
    "sp500_data['Realized_Volatility'] = sp500_data['Log_Returns'] ** 2\n",
    "bitcoin_data['Realized_Volatility'] = bitcoin_data['Log_Returns'] ** 2\n",
    "eurusd_data['Realized_Volatility'] = eurusd_data['Log_Returns'] ** 2\n",
    "\n",
    "# Drop NaN values\n",
    "sp500_clean = sp500_data.dropna()\n",
    "bitcoin_clean = bitcoin_data.dropna()\n",
    "eurusd_clean = eurusd_data.dropna()\n",
    "\n",
    "print(\"\\n=== Data Summary ===\")\n",
    "print(f\"S&P 500 clean data: {len(sp500_clean)} observations\")\n",
    "print(f\"Bitcoin clean data: {len(bitcoin_clean)} observations\")\n",
    "print(f\"EURUSD clean data: {len(eurusd_clean)} observations\")\n",
    "print(f\"\\nS&P 500 Log Returns - Mean: {sp500_clean['Log_Returns'].mean():.6f}, Std: {sp500_clean['Log_Returns'].std():.6f}\")\n",
    "print(f\"Bitcoin Log Returns - Mean: {bitcoin_clean['Log_Returns'].mean():.6f}, Std: {bitcoin_clean['Log_Returns'].std():.6f}\")\n",
    "print(f\"EURUSD Log Returns - Mean: {eurusd_clean['Log_Returns'].mean():.6f}, Std: {eurusd_clean['Log_Returns'].std():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68268dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "\n",
    "ax.plot(eurusd_clean.index, eurusd_clean['Log_Returns'])\n",
    "ax.set_title('EURUSD Logarithmic Returns')\n",
    "ax.set_ylabel('Log Returns')\n",
    "ax.set_xlabel('Date')\n",
    "ax.axhline(y=0, linestyle='--', alpha=0.5)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26fcf41",
   "metadata": {},
   "source": [
    "## CV Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e443573a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.dates as mdates\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "def create_sp500_cv_splits(data, start_date=None):\n",
    "    if start_date is None:\n",
    "        start_date = data.index.min()\n",
    "    \n",
    "    cv_splits = []\n",
    "    window_start = start_date\n",
    "    \n",
    "    while True:\n",
    "        # Define window boundaries\n",
    "        train_start = window_start\n",
    "        train_end = train_start + relativedelta(years=3) - timedelta(days=1)\n",
    "        \n",
    "        # Validation periods (8, 16, 24 months)\n",
    "        val_start = train_end + timedelta(days=1)\n",
    "        val1_end = val_start + relativedelta(months=8) - timedelta(days=1)  # 8 months\n",
    "        val2_end = val_start + relativedelta(months=16) - timedelta(days=1) # 16 months  \n",
    "        val3_end = val_start + relativedelta(months=24) - timedelta(days=1) # 24 months (2 years)\n",
    "        \n",
    "        # Test period (1 year)\n",
    "        test_start = val3_end + timedelta(days=1)\n",
    "        test_end = test_start + relativedelta(years=1) - timedelta(days=1)\n",
    "        \n",
    "        if test_end.year > 2024:\n",
    "            break\n",
    "            \n",
    "        # Create splits for this window\n",
    "        train_data = data[(data.index >= train_start) & (data.index <= train_end)]\n",
    "        \n",
    "        # Three validation folds\n",
    "        val1_data = data[(data.index >= val_start) & (data.index <= val1_end)]\n",
    "        val2_data = data[(data.index >= val_start) & (data.index <= val2_end)]\n",
    "        val3_data = data[(data.index >= val_start) & (data.index <= val3_end)]\n",
    "        \n",
    "        test_data = data[(data.index >= test_start) & (data.index <= test_end)]\n",
    "        \n",
    "        cv_splits.append({\n",
    "            'window_id': len(cv_splits) + 1,\n",
    "            'train': {\n",
    "                'data': train_data,\n",
    "                'start': train_start,\n",
    "                'end': train_end,\n",
    "                'size': len(train_data)\n",
    "            },\n",
    "            'validation': [\n",
    "                {\n",
    "                    'fold': 1,\n",
    "                    'data': val1_data,\n",
    "                    'start': val_start,\n",
    "                    'end': val1_end,\n",
    "                    'size': len(val1_data),\n",
    "                    'months': 8\n",
    "                },\n",
    "                {\n",
    "                    'fold': 2, \n",
    "                    'data': val2_data,\n",
    "                    'start': val_start,\n",
    "                    'end': val2_end,\n",
    "                    'size': len(val2_data),\n",
    "                    'months': 16\n",
    "                },\n",
    "                {\n",
    "                    'fold': 3,\n",
    "                    'data': val3_data,\n",
    "                    'start': val_start,\n",
    "                    'end': val3_end,\n",
    "                    'size': len(val3_data),\n",
    "                    'months': 24\n",
    "                }\n",
    "            ],\n",
    "            'test': {\n",
    "                'data': test_data,\n",
    "                'start': test_start,\n",
    "                'end': test_end,\n",
    "                'size': len(test_data)\n",
    "            }\n",
    "        })\n",
    "        \n",
    "        # Move window forward by 1 year\n",
    "        window_start += relativedelta(years=1)\n",
    "    \n",
    "    return cv_splits\n",
    "\n",
    "def create_bitcoin_cv_splits(data, start_date=None):    \n",
    "    if start_date is None:\n",
    "        # Start from a date that allows for proper window construction\n",
    "        start_date = datetime(2015, 1, 1)\n",
    "    \n",
    "    cv_splits = []\n",
    "    window_start = start_date\n",
    "    \n",
    "    # Define the testing period constraint\n",
    "    test_period_start = datetime(2018, 1, 1)\n",
    "    test_period_end = datetime(2023, 12, 31)\n",
    "    \n",
    "    while True:\n",
    "        # Define window boundaries\n",
    "        train_start = window_start\n",
    "        train_end = train_start + relativedelta(years=2) - timedelta(days=1)\n",
    "        \n",
    "        # Validation periods (4, 8, 12 months)\n",
    "        val_start = train_end + timedelta(days=1)\n",
    "        val1_end = val_start + relativedelta(months=4) - timedelta(days=1)  # 4 months\n",
    "        val2_end = val_start + relativedelta(months=8) - timedelta(days=1)  # 8 months\n",
    "        val3_end = val_start + relativedelta(months=12) - timedelta(days=1) # 12 months\n",
    "        \n",
    "        # Test period (6 months)\n",
    "        test_start = val3_end + timedelta(days=1)\n",
    "        test_end = test_start + relativedelta(months=6) - timedelta(days=1)\n",
    "        \n",
    "        if test_end.year > 2023:\n",
    "            break\n",
    "        \n",
    "        # Only include windows where test period is within 2018-2023\n",
    "        if test_start < test_period_start:\n",
    "            window_start += relativedelta(months=6)\n",
    "            continue\n",
    "            \n",
    "        # Create splits for this window\n",
    "        train_data = data[(data.index >= train_start) & (data.index <= train_end)]\n",
    "        \n",
    "        # Three validation folds\n",
    "        val1_data = data[(data.index >= val_start) & (data.index <= val1_end)]\n",
    "        val2_data = data[(data.index >= val_start) & (data.index <= val2_end)]\n",
    "        val3_data = data[(data.index >= val_start) & (data.index <= val3_end)]\n",
    "        \n",
    "        test_data = data[(data.index >= test_start) & (data.index <= test_end)]\n",
    "        \n",
    "        cv_splits.append({\n",
    "            'window_id': len(cv_splits) + 1,\n",
    "            'train': {\n",
    "                'data': train_data,\n",
    "                'start': train_start,\n",
    "                'end': train_end,\n",
    "                'size': len(train_data)\n",
    "            },\n",
    "            'validation': [\n",
    "                {\n",
    "                    'fold': 1,\n",
    "                    'data': val1_data,\n",
    "                    'start': val_start,\n",
    "                    'end': val1_end,\n",
    "                    'size': len(val1_data),\n",
    "                    'months': 4\n",
    "                },\n",
    "                {\n",
    "                    'fold': 2,\n",
    "                    'data': val2_data, \n",
    "                    'start': val_start,\n",
    "                    'end': val2_end,\n",
    "                    'size': len(val2_data),\n",
    "                    'months': 8\n",
    "                },\n",
    "                {\n",
    "                    'fold': 3,\n",
    "                    'data': val3_data,\n",
    "                    'start': val_start,\n",
    "                    'end': val3_end,\n",
    "                    'size': len(val3_data),\n",
    "                    'months': 12\n",
    "                }\n",
    "            ],\n",
    "            'test': {\n",
    "                'data': test_data,\n",
    "                'start': test_start,\n",
    "                'end': test_end,\n",
    "                'size': len(test_data)\n",
    "            }\n",
    "        })\n",
    "        \n",
    "        # Move window forward by 6 months\n",
    "        window_start += relativedelta(months=6)\n",
    "    \n",
    "    return cv_splits\n",
    "\n",
    "def create_eurusd_cv_splits(data, start_date=None):\n",
    "    \n",
    "    if start_date is None:\n",
    "        start_date = data.index.min()\n",
    "    \n",
    "    cv_splits = []\n",
    "    window_start = start_date\n",
    "    \n",
    "    while True:\n",
    "        # Define window boundaries\n",
    "        train_start = window_start\n",
    "        train_end = train_start + relativedelta(years=2) - timedelta(days=1)\n",
    "        \n",
    "        # Validation periods (8, 16, 24 months)\n",
    "        val_start = train_end + timedelta(days=1)\n",
    "        val1_end = val_start + relativedelta(months=8) - timedelta(days=1)\n",
    "        val2_end = val_start + relativedelta(months=16) - timedelta(days=1)\n",
    "        val3_end = val_start + relativedelta(months=24) - timedelta(days=1)\n",
    "        \n",
    "        # Test period (6 months)\n",
    "        test_start = val3_end + timedelta(days=1)\n",
    "        test_end = test_start + relativedelta(months=6) - timedelta(days=1)\n",
    "        \n",
    "        if test_end > data.index.max():\n",
    "            break\n",
    "            \n",
    "        # Create splits for this window\n",
    "        train_data = data[(data.index >= train_start) & (data.index <= train_end)]\n",
    "        \n",
    "        # Three validation folds\n",
    "        val1_data = data[(data.index >= val_start) & (data.index <= val1_end)]\n",
    "        val2_data = data[(data.index >= val_start) & (data.index <= val2_end)]\n",
    "        val3_data = data[(data.index >= val_start) & (data.index <= val3_end)]\n",
    "        \n",
    "        test_data = data[(data.index >= test_start) & (data.index <= test_end)]\n",
    "        \n",
    "        cv_splits.append({\n",
    "            'window_id': len(cv_splits) + 1,\n",
    "            'train': {\n",
    "                'data': train_data,\n",
    "                'start': train_start,\n",
    "                'end': train_end,\n",
    "                'size': len(train_data)\n",
    "            },\n",
    "            'validation': [\n",
    "                {\n",
    "                    'fold': 1,\n",
    "                    'data': val1_data,\n",
    "                    'start': val_start,\n",
    "                    'end': val1_end,\n",
    "                    'size': len(val1_data),\n",
    "                    'months': 8\n",
    "                },\n",
    "                {\n",
    "                    'fold': 2,\n",
    "                    'data': val2_data,\n",
    "                    'start': val_start,\n",
    "                    'end': val2_end,\n",
    "                    'size': len(val2_data),\n",
    "                    'months': 16\n",
    "                },\n",
    "                {\n",
    "                    'fold': 3,\n",
    "                    'data': val3_data,\n",
    "                    'start': val_start,\n",
    "                    'end': val3_end,\n",
    "                    'size': len(val3_data),\n",
    "                    'months': 24\n",
    "                }\n",
    "            ],\n",
    "            'test': {\n",
    "                'data': test_data,\n",
    "                'start': test_start,\n",
    "                'end': test_end,\n",
    "                'size': len(test_data)\n",
    "            }\n",
    "        })\n",
    "        \n",
    "        # Move window forward by 1 year\n",
    "        window_start += relativedelta(years=1)\n",
    "    \n",
    "    return cv_splits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fd8467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CV splits\n",
    "sp500_cv_splits = create_sp500_cv_splits(sp500_clean)\n",
    "bitcoin_cv_splits = create_bitcoin_cv_splits(bitcoin_clean)\n",
    "eurusd_cv_splits = create_eurusd_cv_splits(eurusd_clean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a828365a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Cross-Validation Scheme\n",
    "\n",
    "def plot_cv_timeline(cv_splits, asset_name, max_windows=8):\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(16, max(6, len(cv_splits[:max_windows]) * 1.5)))\n",
    "    \n",
    "    # Colors for different split types\n",
    "    colors = {\n",
    "        'train': '#2E8B57', \n",
    "        'val_fold1': '#4169E1',  \n",
    "        'val_fold2': '#1E90FF',  \n",
    "        'val_fold3': '#87CEEB',  \n",
    "        'test': '#DC143C'        \n",
    "    }\n",
    "    \n",
    "    y_positions = []\n",
    "    \n",
    "    for i, split in enumerate(cv_splits[:max_windows]):\n",
    "        y_pos = len(cv_splits[:max_windows]) - i - 1\n",
    "        y_positions.append(y_pos)\n",
    "        \n",
    "        # Plot training period\n",
    "        ax.barh(y_pos, (split['train']['end'] - split['train']['start']).days, \n",
    "                left=split['train']['start'], height=0.6, \n",
    "                color=colors['train'], alpha=0.8, label='Train' if i == 0 else \"\")\n",
    "        \n",
    "        # Plot validation periods\n",
    "        val_colors = ['val_fold1', 'val_fold2', 'val_fold3']\n",
    "        for j, val_fold in enumerate(split['validation']):\n",
    "            ax.barh(y_pos + 0.1 + j*0.15, (val_fold['end'] - val_fold['start']).days,\n",
    "                    left=val_fold['start'], height=0.12,\n",
    "                    color=colors[val_colors[j]], alpha=0.8,\n",
    "                    label=f'Val Fold {j+1} ({val_fold[\"months\"]}mo)' if i == 0 else \"\")\n",
    "        \n",
    "        # Plot test period\n",
    "        ax.barh(y_pos, (split['test']['end'] - split['test']['start']).days,\n",
    "                left=split['test']['start'], height=0.6,\n",
    "                color=colors['test'], alpha=0.8, label='Test' if i == 0 else \"\")\n",
    "        \n",
    "        # Add window labels\n",
    "        ax.text(split['train']['start'], y_pos, f'W{split[\"window_id\"]}',\n",
    "                verticalalignment='center', fontsize=9, fontweight='bold')\n",
    "    \n",
    "    # Formatting\n",
    "    ax.set_ylim(-0.5, len(cv_splits[:max_windows]) - 0.5)\n",
    "    ax.set_ylabel('CV Windows (Newest to Oldest)', fontsize=12)\n",
    "    ax.set_xlabel('Time Period', fontsize=12)\n",
    "    ax.set_title(f'{asset_name} Cross-Validation Timeline\\n({len(cv_splits)} Total Windows, Showing First {min(max_windows, len(cv_splits))})', \n",
    "                fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Format x-axis\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
    "    ax.xaxis.set_major_locator(mdates.YearLocator())\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Add legend\n",
    "    ax.legend(loc='upper right', bbox_to_anchor=(1, 1), frameon=True, fancybox=True, shadow=True)\n",
    "    \n",
    "    # Add grid\n",
    "    ax.grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig, ax\n",
    "\n",
    "\n",
    "# Create visualizations\n",
    "fig1, ax1 = plot_cv_timeline(sp500_cv_splits, 'S&P 500', max_windows=8)\n",
    "fig1, ax2 = plot_cv_timeline(bitcoin_cv_splits, 'Bitcoin', max_windows=8)\n",
    "fig1, ax3 = plot_cv_timeline(eurusd_cv_splits, 'EURUSD', max_windows=8)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## GARCH Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_garch_model(returns, p=1, q=1):\n",
    "    returns_scaled = returns * 100\n",
    "    model = arch_model(returns_scaled, vol='Garch', p=p, q=q, rescale=False)\n",
    "    model_fit = model.fit(disp='off')\n",
    "    \n",
    "    return model_fit\n",
    "\n",
    "def get_garch_volatility(model_fit, returns):\n",
    "    conditional_vol = model_fit.conditional_volatility / 100\n",
    "    \n",
    "    return conditional_vol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af4e6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_garch_cross_validation(cv_splits, data_clean, asset_name, max_p=3, max_q=3, \n",
    "                               information_criterion='aic', verbose=True):\n",
    "\n",
    "    ic_attr = information_criterion.lower()\n",
    "    if ic_attr not in ['aic', 'bic', 'hqic']:\n",
    "        raise ValueError(\"information_criterion must be one of: 'aic', 'bic', 'hqic'\")\n",
    "\n",
    "    print(f\"\\n=== {asset_name.upper()} GARCH CROSS-VALIDATION ===\")\n",
    "    print(f\"Running {ic_attr.upper()}-based model selection across {len(cv_splits)} windows...\")\n",
    "    print(f\"Parameter search space: p∈[1,{max_p}], q∈[1,{max_q}]\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    all_results = []\n",
    "    model_selection_summary = []\n",
    "\n",
    "    for window_idx, split in enumerate(cv_splits):\n",
    "        window_id = split['window_id']\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\n Processing Window {window_id}/{len(cv_splits)}...\")\n",
    "            print(f\"   Train: {split['train']['start'].strftime('%Y-%m-%d')} \"\n",
    "                  f\"to {split['train']['end'].strftime('%Y-%m-%d')} \"\n",
    "                  f\"({split['train']['size']} obs)\")\n",
    "            print(f\"   Test:  {split['test']['start'].strftime('%Y-%m-%d')} \"\n",
    "                  f\"to {split['test']['end'].strftime('%Y-%m-%d')} \"\n",
    "                  f\"({split['test']['size']} obs)\")\n",
    "\n",
    "        # Extract returns \n",
    "        train_data = split['train']['data']['Log_Returns'].copy()\n",
    "        test_data = split['test']['data']['Log_Returns'].copy()\n",
    "\n",
    "        #  Model Selection on Train via IC\n",
    "        if verbose:\n",
    "            print(f\"    GARCH model selection using {ic_attr.upper()}...\")\n",
    "\n",
    "        best_ic = np.inf\n",
    "        best_order = None\n",
    "        best_model_fit = None\n",
    "\n",
    "        for p in range(1, max_p + 1):\n",
    "            for q in range(1, max_q + 1):\n",
    "                try:\n",
    "                    returns_scaled = train_data * 100\n",
    "                    model = arch_model(returns_scaled, vol='Garch', p=p, q=q, rescale=False)\n",
    "                    model_fit = model.fit(disp='off')\n",
    "\n",
    "                    ic_value = getattr(model_fit, ic_attr)\n",
    "\n",
    "                    if np.isfinite(ic_value) and ic_value < best_ic:\n",
    "                        best_ic = ic_value\n",
    "                        best_order = (p, q)\n",
    "                        best_model_fit = model_fit\n",
    "\n",
    "                except Exception as e:\n",
    "                    if verbose:\n",
    "                        print(f\"       GARCH({p},{q}) failed: {str(e)[:60]}...\")\n",
    "                    continue\n",
    "\n",
    "        if best_model_fit is None:\n",
    "            print(f\"    Failed to find suitable GARCH model for Window {window_id}\")\n",
    "            continue\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"    Selected GARCH{best_order} with {ic_attr.upper()} = {best_ic:.4f}\")\n",
    "\n",
    "        #  Validation folds (rolling forecasts) \n",
    "        if verbose:\n",
    "            print(f\"    Validating GARCH{best_order} across 3 validation folds...\")\n",
    "\n",
    "        validation_scores = []\n",
    "\n",
    "        for val_fold in split['validation']:\n",
    "            fold_num = val_fold['fold']\n",
    "            val_data = val_fold['data']['Log_Returns'].copy()\n",
    "\n",
    "            # If the fold is empty, skip\n",
    "            if len(val_data) == 0:\n",
    "                if verbose:\n",
    "                    print(f\"       Validation fold {fold_num} is empty, skipping.\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                history = train_data.copy()\n",
    "                val_forecast_vol = []\n",
    "\n",
    "                # Rolling 1-step-ahead forecasts through the validation period\n",
    "                for t_idx, (val_date, ret) in enumerate(val_data.items()):\n",
    "                    model_fit_t = fit_garch_model(history, p=best_order[0], q=best_order[1])\n",
    "                    cond_vol_t = get_garch_volatility(model_fit_t, history)\n",
    "                    forecast_vol_t = cond_vol_t.iloc[-1]  # one-step-ahead for this date\n",
    "\n",
    "                    val_forecast_vol.append(forecast_vol_t)\n",
    "\n",
    "                    # Expand history to include this observed return\n",
    "                    history = pd.concat([history, pd.Series([ret], index=[val_date])])\n",
    "\n",
    "                val_forecast_vol = pd.Series(val_forecast_vol, index=val_data.index)\n",
    "                realized_vol_val = np.abs(val_data)\n",
    "\n",
    "                mask_val = val_forecast_vol.notna() & realized_vol_val.notna()\n",
    "                if mask_val.sum() == 0:\n",
    "                    if verbose:\n",
    "                        print(f\"       No valid forecasts in validation fold {fold_num}\")\n",
    "                    val_rmse = np.inf\n",
    "                else:\n",
    "                    val_rmse = np.sqrt(mean_squared_error(realized_vol_val[mask_val],\n",
    "                                                          val_forecast_vol[mask_val]))\n",
    "\n",
    "                validation_scores.append(val_rmse)\n",
    "\n",
    "                if verbose:\n",
    "                    print(f\"      Fold {fold_num}: Validation RMSE={val_rmse:.6f}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                if verbose:\n",
    "                    print(f\"       Validation fold {fold_num} failed: {str(e)[:60]}...\")\n",
    "                validation_scores.append(np.inf)\n",
    "\n",
    "        avg_validation_rmse = (np.mean(validation_scores)\n",
    "                               if len(validation_scores) > 0\n",
    "                               else np.inf)\n",
    "\n",
    "        # Final test evaluation (rolling forecasts) \n",
    "        if verbose:\n",
    "            print(\"    Final evaluation on test data...\")\n",
    "\n",
    "        try:\n",
    "            history = train_data.copy()\n",
    "            test_forecast_vol = []\n",
    "\n",
    "            for t_idx, (test_date, ret) in enumerate(test_data.items()):\n",
    "                model_fit_t = fit_garch_model(history, p=best_order[0], q=best_order[1])\n",
    "                cond_vol_t = get_garch_volatility(model_fit_t, history)\n",
    "                forecast_vol_t = cond_vol_t.iloc[-1]\n",
    "\n",
    "                test_forecast_vol.append(forecast_vol_t)\n",
    "                history = pd.concat([history, pd.Series([ret], index=[test_date])])\n",
    "\n",
    "            test_forecast_vol = pd.Series(test_forecast_vol, index=test_data.index)\n",
    "            realized_vol_test = np.abs(test_data)\n",
    "\n",
    "            mask_test = test_forecast_vol.notna() & realized_vol_test.notna()\n",
    "            if mask_test.sum() == 0:\n",
    "                print(f\"    No valid test forecasts for Window {window_id}\")\n",
    "                continue\n",
    "\n",
    "            test_rmse = np.sqrt(mean_squared_error(realized_vol_test[mask_test],\n",
    "                                                   test_forecast_vol[mask_test]))\n",
    "            test_mae = mean_absolute_error(realized_vol_test[mask_test],\n",
    "                                           test_forecast_vol[mask_test])\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"   GARCH{best_order}: Test RMSE={test_rmse:.6f}, Test MAE={test_mae:.6f}\")\n",
    "\n",
    "            # In-sample train metrics from the selected model\n",
    "            train_cond_vol = get_garch_volatility(best_model_fit, train_data)\n",
    "            realized_vol_train = np.abs(train_data)\n",
    "\n",
    "            train_rmse = np.sqrt(mean_squared_error(realized_vol_train, train_cond_vol))\n",
    "            train_mae = mean_absolute_error(realized_vol_train, train_cond_vol)\n",
    "\n",
    "            # Store detailed window result\n",
    "            window_result = {\n",
    "                'window_id': window_id,\n",
    "                'asset': asset_name,\n",
    "                'train_period': f\"{split['train']['start'].strftime('%Y-%m-%d')} \"\n",
    "                                f\"to {split['train']['end'].strftime('%Y-%m-%d')}\",\n",
    "                'test_period': f\"{split['test']['start'].strftime('%Y-%m-%d')} \"\n",
    "                               f\"to {split['test']['end'].strftime('%Y-%m-%d')}\",\n",
    "                'train_size': split['train']['size'],\n",
    "                'test_size': split['test']['size'],\n",
    "                'best_order': best_order,\n",
    "                'best_ic_value': best_ic,\n",
    "                'validation_scores': validation_scores,\n",
    "                'avg_validation_rmse': avg_validation_rmse,\n",
    "                'train_cond_vol': train_cond_vol,\n",
    "                'test_forecast_vol': test_forecast_vol,\n",
    "                'realized_vol_train': realized_vol_train,\n",
    "                'realized_vol_test': realized_vol_test,\n",
    "                'metrics': {\n",
    "                    'train_rmse': train_rmse,\n",
    "                    'train_mae': train_mae,\n",
    "                    'test_rmse': test_rmse,\n",
    "                    'test_mae': test_mae\n",
    "                },\n",
    "                'final_model': best_model_fit\n",
    "            }\n",
    "\n",
    "            all_results.append(window_result)\n",
    "\n",
    "            model_selection_summary.append({\n",
    "                'Window': window_id,\n",
    "                'Best_Order': f\"GARCH{best_order}\",\n",
    "                ic_attr.upper(): best_ic,\n",
    "                'Validation_RMSE': avg_validation_rmse,\n",
    "                'Train_RMSE': train_rmse,\n",
    "                'Train_MAE': train_mae,\n",
    "                'Test_RMSE': test_rmse,\n",
    "                'Test_MAE': test_mae\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"   Final evaluation failed for Window {window_id}: {str(e)[:80]}\")\n",
    "            continue\n",
    "\n",
    "    summary_df = pd.DataFrame(model_selection_summary)\n",
    "\n",
    "    if len(summary_df) > 0:\n",
    "        performance_summary = {\n",
    "            'total_windows': len(cv_splits),\n",
    "            'successful_windows': len(summary_df),\n",
    "            'success_rate': len(summary_df) / len(cv_splits) * 100,\n",
    "            'avg_test_rmse': summary_df['Test_RMSE'].mean(),\n",
    "            'std_test_rmse': summary_df['Test_RMSE'].std(),\n",
    "            'avg_test_mae': summary_df['Test_MAE'].mean(),\n",
    "            'avg_train_rmse': summary_df['Train_RMSE'].mean(),\n",
    "            'avg_train_mae': summary_df['Train_MAE'].mean(),\n",
    "            'avg_validation_rmse': summary_df['Validation_RMSE'].mean(),\n",
    "            'most_common_order': summary_df['Best_Order'].mode().iloc[0]\n",
    "                                 if len(summary_df) > 0 else None\n",
    "        }\n",
    "    else:\n",
    "        performance_summary = None\n",
    "\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"{asset_name.upper()} GARCH CROSS-VALIDATION COMPLETE\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "    if performance_summary:\n",
    "        print(f\"Successfully processed {performance_summary['successful_windows']}/\"\n",
    "              f\"{performance_summary['total_windows']} windows\")\n",
    "        print(f\"Average Test RMSE: {performance_summary['avg_test_rmse']:.6f} \"\n",
    "              f\"± {performance_summary['std_test_rmse']:.6f}\")\n",
    "        print(f\"Average Validation RMSE: {performance_summary['avg_validation_rmse']:.6f}\")\n",
    "        print(f\" Most Common Model: {performance_summary['most_common_order']}\")\n",
    "    else:\n",
    "        print(\" No successful model fits achieved\")\n",
    "\n",
    "    return {\n",
    "        'asset_name': asset_name,\n",
    "        'all_results': all_results,\n",
    "        'summary_df': summary_df,\n",
    "        'performance_summary': performance_summary,\n",
    "        'methodology': {\n",
    "            'approach': f'{ic_attr.upper()}-based GARCH(p, q) selection',\n",
    "            'information_criterion': information_criterion,\n",
    "            'parameter_space': f'p∈[1,{max_p}], q∈[1,{max_q}]',\n",
    "            'cross_validation': '3-fold temporal validation with rolling 1-step-ahead forecasts',\n",
    "            'evaluation_metric': 'RMSE and MAE between predicted and realized volatility (|returns|)'\n",
    "        }\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f52c262",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp500_cv_splits = create_sp500_cv_splits(sp500_clean)\n",
    "bitcoin_cv_splits = create_bitcoin_cv_splits(bitcoin_clean)\n",
    "eurusd_cv_splits = create_eurusd_cv_splits(eurusd_clean)\n",
    "\n",
    "garch_results_sp = run_garch_cross_validation(sp500_cv_splits, sp500_clean, 'S&P 500')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff75bbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "garch_results_btc = run_garch_cross_validation(bitcoin_cv_splits, bitcoin_clean, 'Bitcoin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e14dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "garch_results_eur = run_garch_cross_validation(eurusd_cv_splits, eurusd_clean, 'EURUSD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1534b301",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_garch_results_summary(sp500_results, bitcoin_results,eurusd_results):\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"COMPREHENSIVE GARCH ANALYSIS RESULTS\")  \n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    summary_stats = {\n",
    "        'Asset': ['S&P 500', 'Bitcoin','EURUSD'],\n",
    "        'Total_Windows': [\n",
    "            sp500_results['performance_summary']['total_windows'] \n",
    "            if sp500_results['performance_summary'] else 0,\n",
    "            bitcoin_results['performance_summary']['total_windows'] \n",
    "            if bitcoin_results['performance_summary'] else 0,\n",
    "            eurusd_results['performance_summary']['total_windows'] \n",
    "            if eurusd_results['performance_summary'] else 0\n",
    "        ],\n",
    "        'Avg_Train_RMSE': [\n",
    "            sp500_results['performance_summary']['avg_train_rmse'] \n",
    "            if sp500_results['performance_summary'] else np.nan,\n",
    "            bitcoin_results['performance_summary']['avg_train_rmse'] \n",
    "            if bitcoin_results['performance_summary'] else np.nan,\n",
    "            eurusd_results['performance_summary']['avg_train_rmse'] \n",
    "            if eurusd_results['performance_summary'] else np.nan\n",
    "        ],\n",
    "        'Avg_Validation_RMSE': [\n",
    "            sp500_results['performance_summary']['avg_validation_rmse'] \n",
    "            if sp500_results['performance_summary'] else np.nan,\n",
    "            bitcoin_results['performance_summary']['avg_validation_rmse'] \n",
    "            if bitcoin_results['performance_summary'] else np.nan,\n",
    "            eurusd_results['performance_summary']['avg_validation_rmse'] \n",
    "            if eurusd_results['performance_summary'] else np.nan\n",
    "        ],\n",
    "        'Avg_Test_RMSE': [\n",
    "            sp500_results['performance_summary']['avg_test_rmse'] \n",
    "            if sp500_results['performance_summary'] else np.nan,\n",
    "            bitcoin_results['performance_summary']['avg_test_rmse'] \n",
    "            if bitcoin_results['performance_summary'] else np.nan,\n",
    "            eurusd_results['performance_summary']['avg_test_rmse'] \n",
    "            if eurusd_results['performance_summary'] else np.nan\n",
    "        ],\n",
    "        'Avg_Test_MAE': [\n",
    "            sp500_results['performance_summary']['avg_test_mae'] \n",
    "            if sp500_results['performance_summary'] else np.nan,\n",
    "            bitcoin_results['performance_summary']['avg_test_mae'] \n",
    "            if bitcoin_results['performance_summary'] else np.nan,\n",
    "            eurusd_results['performance_summary']['avg_test_mae'] \n",
    "            if eurusd_results['performance_summary'] else np.nan\n",
    "        ],\n",
    "        'Most_Common_Model': [\n",
    "            sp500_results['performance_summary']['most_common_order'] \n",
    "            if sp500_results['performance_summary'] else 'N/A',\n",
    "            bitcoin_results['performance_summary']['most_common_order'] \n",
    "            if bitcoin_results['performance_summary'] else 'N/A',\n",
    "            eurusd_results['performance_summary']['most_common_order'] \n",
    "            if eurusd_results['performance_summary'] else 'N/A'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_stats)\n",
    "    \n",
    "    print(\"\\n OVERALL PERFORMANCE SUMMARY (GARCH)\")\n",
    "    print(\"-\" * 50)\n",
    "    print(summary_df.to_string(index=False, float_format='%.6f'))\n",
    "    \n",
    "    # Detailed window-by-window results\n",
    "    if len(sp500_results['summary_df']) > 0:\n",
    "        print(f\"\\n S&P 500 GARCH DETAILED RESULTS ({len(sp500_results['summary_df'])} windows)\")\n",
    "        print(\"-\" * 50)\n",
    "        print(sp500_results['summary_df'].round(6).to_string(index=False))\n",
    "    \n",
    "    if len(bitcoin_results['summary_df']) > 0:\n",
    "        print(f\"\\n BITCOIN GARCH DETAILED RESULTS ({len(bitcoin_results['summary_df'])} windows)\")\n",
    "        print(\"-\" * 50)\n",
    "        print(bitcoin_results['summary_df'].round(6).to_string(index=False))\n",
    "    \n",
    "    if len(eurusd_results['summary_df']) > 0:\n",
    "        print(f\"\\n EURUSD GARCH DETAILED RESULTS ({len(eurusd_results['summary_df'])} windows)\")\n",
    "        print(\"-\" * 50)\n",
    "        print(eurusd_results['summary_df'].round(6).to_string(index=False))\n",
    "    \n",
    "    return summary_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97646047",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_garch_performance_analysis(sp500_results, bitcoin_results,eurusd_results):    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    fig.suptitle('GARCH Model Performance Analysis\\nConditional Volatility Forecasting', \n",
    "                 fontsize=16, fontweight='bold')\n",
    "    \n",
    "    sp500_df = sp500_results['summary_df'] if len(sp500_results['summary_df']) > 0 else pd.DataFrame()\n",
    "    bitcoin_df = bitcoin_results['summary_df'] if len(bitcoin_df := bitcoin_results['summary_df']) > 0 else pd.DataFrame()\n",
    "    eurusd_df = eurusd_results['summary_df'] if len(eurusd_results['summary_df']) > 0 else pd.DataFrame()\n",
    "\n",
    "    # Plot 1: Test RMSE Comparison\n",
    "    ax1 = axes[0, 0]\n",
    "    if len(sp500_df) > 0 and len(bitcoin_df) > 0 and len(eurusd_df) > 0:\n",
    "        ax1.boxplot(\n",
    "            [sp500_df['Test_RMSE'], bitcoin_df['Test_RMSE'],eurusd_df['Test_RMSE']], \n",
    "            labels=['S&P 500', 'Bitcoin','EURUSD'], \n",
    "            patch_artist=True\n",
    "        )\n",
    "        ax1.set_title('Test RMSE Distribution (GARCH)')\n",
    "        ax1.set_ylabel('RMSE')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Validation RMSE Comparison  \n",
    "    ax2 = axes[0, 1]\n",
    "    if len(sp500_df) > 0 and len(bitcoin_df) > 0 and len(eurusd_df) > 0:\n",
    "        ax2.boxplot(\n",
    "            [sp500_df['Validation_RMSE'], bitcoin_df['Validation_RMSE'],eurusd_df['Validation_RMSE']], \n",
    "            labels=['S&P 500', 'Bitcoin','EURUSD'], \n",
    "            patch_artist=True\n",
    "        )\n",
    "        ax2.set_title('Validation RMSE Distribution')\n",
    "        ax2.set_ylabel('RMSE')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: RMSE over Windows (S&P 500)\n",
    "    ax3 = axes[0,2]\n",
    "    if len(sp500_df) > 0:\n",
    "        ax3.plot(sp500_df['Window'], sp500_df['Test_RMSE'], 'o-', alpha=0.7, label='Test RMSE')\n",
    "        ax3.plot(sp500_df['Window'], sp500_df['Validation_RMSE'], 's--', alpha=0.7, label='Validation RMSE')\n",
    "        ax3.set_title('S&P 500 RMSE Evolution (GARCH)')\n",
    "        ax3.set_xlabel('Window')\n",
    "        ax3.set_ylabel('RMSE')\n",
    "        ax3.legend()\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot 4: RMSE over Windows (Bitcoin)\n",
    "    ax4 = axes[1, 0]  \n",
    "    if len(bitcoin_df) > 0:\n",
    "        ax4.plot(bitcoin_df['Window'], bitcoin_df['Test_RMSE'], 'o-', alpha=0.7, label='Test RMSE')\n",
    "        ax4.plot(bitcoin_df['Window'], bitcoin_df['Validation_RMSE'], 's--', alpha=0.7, label='Validation RMSE')\n",
    "        ax4.set_title('Bitcoin RMSE Evolution (GARCH)')\n",
    "        ax4.set_xlabel('Window')\n",
    "        ax4.set_ylabel('RMSE')\n",
    "        ax4.legend()\n",
    "        ax4.grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot 5: RMSE over Windows (EURUSD)\n",
    "    ax5 = axes[1, 1]  \n",
    "    if len(eurusd_df) > 0:\n",
    "        ax5.plot(eurusd_df['Window'], eurusd_df['Test_RMSE'], 'o-', alpha=0.7, label='Test RMSE')\n",
    "        ax5.plot(eurusd_df['Window'], eurusd_df['Validation_RMSE'], 's--', alpha=0.7, label='Validation RMSE')\n",
    "        ax5.set_title('EURUSD RMSE Evolution (GARCH)')\n",
    "        ax5.set_xlabel('Window')\n",
    "        ax5.set_ylabel('RMSE')\n",
    "        ax5.legend()\n",
    "        ax5.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 6: Train vs Test RMSE (per asset, bar)\n",
    "    ax6 = axes[1, 2]\n",
    "    if sp500_results['performance_summary'] and bitcoin_results['performance_summary'] and eurusd_results['performance_summary']:\n",
    "        assets = ['S&P 500', 'Bitcoin','EURUSD']\n",
    "        train_vals = [\n",
    "            sp500_results['performance_summary']['avg_train_rmse'],\n",
    "            bitcoin_results['performance_summary']['avg_train_rmse'],\n",
    "            eurusd_results['performance_summary']['avg_train_rmse']\n",
    "        ]\n",
    "        test_vals = [\n",
    "            sp500_results['performance_summary']['avg_test_rmse'],\n",
    "            bitcoin_results['performance_summary']['avg_test_rmse'],\n",
    "            eurusd_results['performance_summary']['avg_test_rmse']\n",
    "        ]\n",
    "        \n",
    "        x = np.arange(len(assets))\n",
    "        width = 0.35\n",
    "        \n",
    "        bars1 = ax6.bar(x - width/2, train_vals, width, label='Avg Train RMSE', alpha=0.7)\n",
    "        bars2 = ax6.bar(x + width/2, test_vals, width, label='Avg Test RMSE', alpha=0.7)\n",
    "        \n",
    "        ax6.set_xticks(x)\n",
    "        ax6.set_xticklabels(assets)\n",
    "        ax6.set_title('Average Train vs Test RMSE (GARCH)')\n",
    "        ax6.set_ylabel('RMSE')\n",
    "        ax6.legend()\n",
    "        ax6.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Value labels\n",
    "        for bars in [bars1, bars2]:\n",
    "            for bar in bars:\n",
    "                height = bar.get_height()\n",
    "                ax6.text(\n",
    "                    bar.get_x() + bar.get_width()/2., \n",
    "                    height, \n",
    "                    f'{height:.4f}', \n",
    "                    ha='center', va='bottom', fontsize=8\n",
    "                )\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9268bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "garch_summary_df = create_garch_results_summary(garch_results_sp, garch_results_btc,garch_results_eur)\n",
    "fig_garch = plot_garch_performance_analysis(garch_results_sp, garch_results_btc,garch_results_eur)\n",
    "plt.show()\n",
    "\n",
    "create_garch_results_summary(garch_results_sp, garch_results_btc,garch_results_eur)\n",
    "\n",
    "print(\"Successfully implemented IC-based GARCH methodology\")\n",
    "print(f\"Processed {len(sp500_cv_splits) + len(bitcoin_cv_splits) + len(eurusd_cv_splits)} total cross-validation windows (GARCH)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Fitting GARCH Models ===\")\n",
    "print(\"\\nFitting GARCH(1,1) for S&P 500...\")\n",
    "sp500_garch = fit_garch_model(sp500_clean['Log_Returns'])\n",
    "print(sp500_garch.summary())\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nFitting GARCH(1,1) for Bitcoin...\")\n",
    "bitcoin_garch = fit_garch_model(bitcoin_clean['Log_Returns'])\n",
    "print(bitcoin_garch.summary())\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nFitting GARCH(1,1) for EURUSD...\")\n",
    "eurusd_garch = fit_garch_model(eurusd_clean['Log_Returns'])\n",
    "print(eurusd_garch.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp500_clean['GARCH_Volatility'] = get_garch_volatility(sp500_garch, sp500_clean['Log_Returns'])\n",
    "bitcoin_clean['GARCH_Volatility'] = get_garch_volatility(bitcoin_garch, bitcoin_clean['Log_Returns'])\n",
    "eurusd_clean['GARCH_Volatility'] = get_garch_volatility(eurusd_garch, eurusd_clean['Log_Returns'])\n",
    "\n",
    "print(\"\\n GARCH Volatility Statistics \")\n",
    "print(f\"S&P 500 GARCH Volatility - Mean: {sp500_clean['GARCH_Volatility'].mean():.6f}, Std: {sp500_clean['GARCH_Volatility'].std():.6f}\")\n",
    "print(f\"Bitcoin GARCH Volatility - Mean: {bitcoin_clean['GARCH_Volatility'].mean():.6f}, Std: {bitcoin_clean['GARCH_Volatility'].std():.6f}\")\n",
    "print(f\"EURUSD GARCH Volatility - Mean: {eurusd_clean['GARCH_Volatility'].mean():.6f}, Std: {eurusd_clean['GARCH_Volatility'].std():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446084a2",
   "metadata": {},
   "source": [
    "## LSTM CV Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4c97d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM\n",
    "\n",
    "def create_lstm_sequences_multifeature(features, target, lookback=60):\n",
    "\n",
    "    if target.ndim == 2:\n",
    "        target = target.flatten()\n",
    "        \n",
    "    X, y = [], []\n",
    "    for i in range(lookback, len(target)):\n",
    "        X.append(features[i-lookback:i, :])   # (lookback, n_features)\n",
    "        y.append(target[i])                   # scalar\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "def build_lstm_model(lookback, n_features=1, units=50, dropout=0.2):\n",
    "\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(LSTM(units=units, return_sequences=True, input_shape=(lookback, n_features)))\n",
    "    model.add(Dropout(dropout))\n",
    "    \n",
    "    model.add(LSTM(units=units, return_sequences=True))\n",
    "    model.add(Dropout(dropout))\n",
    "    \n",
    "    model.add(LSTM(units=units, return_sequences=False))\n",
    "    model.add(Dropout(dropout))\n",
    "    \n",
    "    model.add(Dense(units=1))\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4adc637a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_garch_lstm_data_for_window(split, garch_window_result, lookback=60):\n",
    "\n",
    "    train_returns = split['train']['data']['Log_Returns']\n",
    "    test_returns  = split['test']['data']['Log_Returns']\n",
    "    \n",
    "    returns_full = pd.concat([train_returns, test_returns])\n",
    "    n_train = len(train_returns)\n",
    "    n_test  = len(test_returns)\n",
    "    T = n_train + n_test\n",
    "    \n",
    "    realized_vol_full = np.abs(returns_full.values) \n",
    "    \n",
    "    train_garch_vol = garch_window_result['train_cond_vol'] \n",
    "    test_garch_vol  = garch_window_result['test_forecast_vol']  \n",
    "\n",
    "    garch_vol_full = pd.concat([train_garch_vol, test_garch_vol])\n",
    "\n",
    "    garch_vol_full = garch_vol_full.loc[returns_full.index].values \n",
    "    \n",
    "    features_full = np.column_stack([realized_vol_full, garch_vol_full])\n",
    "    \n",
    "    scaler_X = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaler_y = MinMaxScaler(feature_range=(0, 1))\n",
    "    \n",
    "    X_scaled = scaler_X.fit_transform(features_full)\n",
    "    y_scaled = scaler_y.fit_transform(realized_vol_full.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    X_seq, y_seq = create_lstm_sequences_multifeature(X_scaled, y_scaled, lookback=lookback)\n",
    "    N_seq = len(y_seq)\n",
    "    \n",
    "\n",
    "    n_train_seq = max(0, n_train - lookback)\n",
    "    \n",
    "    X_train = X_seq[:n_train_seq]\n",
    "    y_train = y_seq[:n_train_seq]\n",
    "    X_test  = X_seq[n_train_seq:]\n",
    "    y_test  = y_seq[n_train_seq:]\n",
    "    \n",
    "    target_indices = np.arange(lookback, T) \n",
    "    \n",
    "    train_target_idx = target_indices[:n_train_seq]\n",
    "    test_target_idx  = target_indices[n_train_seq:]\n",
    "    \n",
    "    y_real_train_orig = realized_vol_full[train_target_idx]\n",
    "    y_real_test_orig  = realized_vol_full[test_target_idx]\n",
    "    \n",
    "    return (X_train, X_test, y_train, y_test, \n",
    "            y_real_train_orig, y_real_test_orig,\n",
    "            scaler_X, scaler_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324b86b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "def run_garch_lstm_cross_validation(cv_splits, garch_results, data_clean, asset_name,\n",
    "                                    lookback=60, units=50, dropout=0.2,\n",
    "                                    epochs=50, batch_size=32, verbose=1):\n",
    "    print(f\"\\n=== {asset_name.upper()} GARCH-LSTM CROSS-VALIDATION ===\")\n",
    "    print(f\"Lookback: {lookback} | Epochs: {epochs} | Batch size: {batch_size}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    all_results = []\n",
    "    summary_rows = []\n",
    "    \n",
    "    garch_window_results = {w['window_id']: w for w in garch_results['all_results']}\n",
    "    \n",
    "    for split in cv_splits:\n",
    "        window_id = split['window_id']\n",
    "        print(f\"\\n Processing Window {window_id}/{len(cv_splits)}...\")\n",
    "        \n",
    "        if window_id not in garch_window_results:\n",
    "            print(f\"    No GARCH results found for Window {window_id}, skipping.\")\n",
    "            continue\n",
    "        \n",
    "        garch_w = garch_window_results[window_id]\n",
    "        \n",
    "        (X_train, X_test, y_train, y_test,\n",
    "         y_real_train_orig, y_real_test_orig,\n",
    "         scaler_X, scaler_y) = prepare_garch_lstm_data_for_window(\n",
    "            split, garch_w, lookback=lookback\n",
    "        )\n",
    "        \n",
    "        if X_train.shape[0] == 0 or X_test.shape[0] == 0:\n",
    "            print(f\"    Not enough sequence data for LSTM in Window {window_id}, skipping.\")\n",
    "            continue\n",
    "        \n",
    "        n_features = X_train.shape[2]\n",
    "        print(f\"   Train sequences: {X_train.shape[0]} | Test sequences: {X_test.shape[0]} | Features: {n_features}\")\n",
    "        \n",
    "        #Build and train model \n",
    "        model = build_lstm_model(lookback=lookback, n_features=n_features,\n",
    "                                 units=units, dropout=dropout)\n",
    "\n",
    "        if verbose:\n",
    "            print(model.summary())\n",
    "        \n",
    "        history = model.fit(\n",
    "            X_train, y_train,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            validation_split=0.1,\n",
    "            verbose=verbose\n",
    "        )\n",
    "        \n",
    "        #Predictions & metrics\n",
    "        y_pred_test_scaled = model.predict(X_test).flatten()\n",
    "        y_pred_test_orig = scaler_y.inverse_transform(\n",
    "            y_pred_test_scaled.reshape(-1, 1)\n",
    "        ).flatten()\n",
    "        \n",
    "        # RMSE / MAE on realized volatility in original scale\n",
    "        test_rmse = np.sqrt(mean_squared_error(y_real_test_orig, y_pred_test_orig))\n",
    "        test_mae = mean_absolute_error(y_real_test_orig, y_pred_test_orig)\n",
    "        \n",
    "        print(f\"    Test RMSE (realized vol) = {test_rmse:.6f}, Test MAE = {test_mae:.6f}\")\n",
    "        \n",
    "        # Store per-window result\n",
    "        window_result = {\n",
    "            'window_id': window_id,\n",
    "            'asset': asset_name,\n",
    "            'train_period': f\"{split['train']['start'].strftime('%Y-%m-%d')} \"\n",
    "                            f\"to {split['train']['end'].strftime('%Y-%m-%d')}\",\n",
    "            'test_period': f\"{split['test']['start'].strftime('%Y-%m-%d')} \"\n",
    "                           f\"to {split['test']['end'].strftime('%Y-%m-%d')}\",\n",
    "            'train_size_obs': split['train']['size'],\n",
    "            'test_size_obs': split['test']['size'],\n",
    "            'train_size_seq': X_train.shape[0],\n",
    "            'test_size_seq': X_test.shape[0],\n",
    "            'garch_order': garch_w['best_order'],\n",
    "            'garch_ic_value': garch_w['best_ic_value'],\n",
    "            'y_predictions': y_pred_test_orig,\n",
    "            'y_actual': y_real_test_orig,\n",
    "            'lookback': lookback,\n",
    "            'lstm_units': units,\n",
    "            'lstm_dropout': dropout,\n",
    "            'metrics': {\n",
    "                'test_rmse_realized_vol': test_rmse,\n",
    "                'test_mae_realized_vol': test_mae\n",
    "            },\n",
    "            'model': model,\n",
    "            'history': history.history,\n",
    "            'scaler_X': scaler_X,\n",
    "            'scaler_y': scaler_y\n",
    "        }\n",
    "        \n",
    "        all_results.append(window_result)\n",
    "        \n",
    "        summary_rows.append({\n",
    "            'Window': window_id,\n",
    "            'GARCH_Order': f\"GARCH{garch_w['best_order']}\",\n",
    "            'Lookback': lookback,\n",
    "            'Train_Seq': X_train.shape[0],\n",
    "            'Test_Seq': X_test.shape[0],\n",
    "            'Test_RMSE_RealVol': test_rmse,\n",
    "            'Test_MAE_RealVol': test_mae\n",
    "        })\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_rows).sort_values('Window')\n",
    "    \n",
    "    if len(summary_df) > 0:\n",
    "        performance_summary = {\n",
    "            'total_windows': len(cv_splits),\n",
    "            'successful_windows': len(summary_df),\n",
    "            'success_rate': len(summary_df) / len(cv_splits) * 100,\n",
    "            'avg_test_rmse_realvol': summary_df['Test_RMSE_RealVol'].mean(),\n",
    "            'std_test_rmse_realvol': summary_df['Test_RMSE_RealVol'].std(),\n",
    "            'avg_test_mae_realvol': summary_df['Test_MAE_RealVol'].mean(),\n",
    "            'std_test_mae_realvol': summary_df['Test_MAE_RealVol'].std()\n",
    "        }\n",
    "    else:\n",
    "        performance_summary = None\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"{asset_name.upper()} GARCH-LSTM CROSS-VALIDATION COMPLETE\")\n",
    "    print(\"=\"*80)\n",
    "    if performance_summary:\n",
    "        print(f\" Successfully processed {performance_summary['successful_windows']}/\"\n",
    "              f\"{performance_summary['total_windows']} windows\")\n",
    "        print(f\" Avg Test RMSE (realized vol): \"\n",
    "              f\"{performance_summary['avg_test_rmse_realvol']:.6f} \"\n",
    "              f\"± {performance_summary['std_test_rmse_realvol']:.6f}\")\n",
    "        print(f\" Avg Test MAE (realized vol): \"\n",
    "              f\"{performance_summary['avg_test_mae_realvol']:.6f} \"\n",
    "              f\"± {performance_summary['std_test_mae_realvol']:.6f}\")\n",
    "    else:\n",
    "        print(\" No successful GARCH-LSTM windows.\")\n",
    "    \n",
    "    return {\n",
    "        'asset_name': asset_name,\n",
    "        'all_results': all_results,\n",
    "        'summary_df': summary_df,\n",
    "        'performance_summary': performance_summary,\n",
    "        'methodology': {\n",
    "            'approach': 'GARCH-LSTM hybrid with CV windows',\n",
    "            'features': '[Realized Volatility, GARCH Conditional Volatility]',\n",
    "            'target': 'Realized Volatility',\n",
    "            'lookback': lookback,\n",
    "            'lstm_architecture': '3-layer LSTM + Dense(1)',\n",
    "            'garch_results_source': 'run_garch_cross_validation outputs'\n",
    "        }\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Train GARCH-LSTM Hybrid Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "lookback = 20\n",
    "\n",
    "garch_lstm_sp = run_garch_lstm_cross_validation(\n",
    "    sp500_cv_splits,\n",
    "    garch_results_sp,\n",
    "    sp500_clean,\n",
    "    asset_name='S&P 500',\n",
    "    lookback=lookback,\n",
    "    units=50,\n",
    "    dropout=0.2,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    verbose=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "lookback = 30\n",
    "\n",
    "garch_lstm_btc = run_garch_lstm_cross_validation(\n",
    "    bitcoin_cv_splits,\n",
    "    garch_results_btc,\n",
    "    bitcoin_clean,\n",
    "    asset_name='Bitcoin',\n",
    "    lookback=lookback,\n",
    "    units=50,\n",
    "    dropout=0.2,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    verbose=0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1a6ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "garch_lstm_eur = run_garch_lstm_cross_validation(\n",
    "    eurusd_cv_splits,\n",
    "    garch_results_eur,\n",
    "    eurusd_clean,\n",
    "    asset_name='EURUSD',\n",
    "    lookback=lookback,\n",
    "    units=50,\n",
    "    dropout=0.2,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    verbose=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## Model Predictions and Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848888d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_garch_vs_garch_lstm(garch_results, garch_lstm_results, asset_label=\"S&P 500\"):\n",
    "    garch_ps = garch_results.get('performance_summary', None)\n",
    "    hybrid_ps = garch_lstm_results.get('performance_summary', None)\n",
    "    \n",
    "    if garch_ps is None or hybrid_ps is None:\n",
    "        print(f\"\\n Missing performance summary for {asset_label}.\")\n",
    "        return\n",
    "    \n",
    "    rmse_garch_test = garch_ps['avg_test_rmse']\n",
    "    mae_garch_test  = garch_ps['avg_test_mae']\n",
    "    \n",
    "    rmse_hybrid_test = hybrid_ps['avg_test_rmse_realvol']\n",
    "    mae_hybrid_test  = hybrid_ps['avg_test_mae_realvol']\n",
    "    \n",
    "    \n",
    "    print(f\"\\n=== {asset_label} Performance Metrics (Cross-Validation Averages) ===\")\n",
    "    \n",
    "    print(\"\\nGARCH-LSTM Hybrid Model (Averaged over windows):\")\n",
    "    print(f\"Test Set - RMSE: {rmse_hybrid_test:.8f}, MAE: {mae_hybrid_test:.8f}\")\n",
    "    \n",
    "    print(\"\\nGARCH Model (Baseline, Averaged over windows):\")\n",
    "    print(f\"Test Set - RMSE: {rmse_garch_test:.8f}, MAE: {mae_garch_test:.8f}\")\n",
    "    \n",
    "    # Improvement (%)\n",
    "    rmse_impr = (rmse_garch_test - rmse_hybrid_test) / rmse_garch_test * 100 if rmse_garch_test != 0 else 0.0\n",
    "    mae_impr  = (mae_garch_test - mae_hybrid_test) / mae_garch_test * 100 if mae_garch_test != 0 else 0.0\n",
    "    \n",
    "    print(\"\\nImprovement over GARCH (Test, Averages):\")\n",
    "    print(f\"Test RMSE Improvement: {rmse_impr:.2f}%\")\n",
    "    print(f\"Test MAE Improvement: {mae_impr:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c8ecd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_garch_vs_garch_lstm(garch_results_sp,  garch_lstm_sp,  asset_label=\"S&P 500\")\n",
    "compare_garch_vs_garch_lstm(garch_results_btc, garch_lstm_btc, asset_label=\"Bitcoin\")\n",
    "compare_garch_vs_garch_lstm(garch_results_eur, garch_lstm_eur, asset_label=\"EURUSD\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history for S&P 500\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(history_sp.history['loss'], label='Training Loss', linewidth=2)\n",
    "plt.plot(history_sp.history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "plt.title('S&P 500: GARCH-LSTM Model Training History', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Loss (MSE)', fontsize=12)\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(history_btc.history['loss'], label='Training Loss', linewidth=2)\n",
    "plt.plot(history_btc.history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "plt.title('Bitcoin: GARCH-LSTM Model Training History', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Loss (MSE)', fontsize=12)\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(history_eurusd.history['loss'], label='Training Loss', linewidth=2)\n",
    "plt.plot(history_eurusd.history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "plt.title('EURUSD: GARCH-LSTM Model Training History', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Loss (MSE)', fontsize=12)\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1931292c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_garch_vs_garch_lstm_bar(\n",
    "    garch_results_sp, garch_results_btc, garch_results_eur,\n",
    "    garch_lstm_sp, garch_lstm_btc, garch_lstm_eur\n",
    "):\n",
    "\n",
    "    datasets = [\"S&P 500\", \"Bitcoin\", \"EURUSD\"]\n",
    "\n",
    "    garch_rmse = [\n",
    "        garch_results_sp['performance_summary']['avg_test_rmse'],\n",
    "        garch_results_btc['performance_summary']['avg_test_rmse'],\n",
    "        garch_results_eur['performance_summary']['avg_test_rmse']\n",
    "    ]\n",
    "\n",
    "    garch_mae = [\n",
    "        garch_results_sp['performance_summary']['avg_test_mae'],\n",
    "        garch_results_btc['performance_summary']['avg_test_mae'],\n",
    "        garch_results_eur['performance_summary']['avg_test_mae']\n",
    "    ]\n",
    "\n",
    "    hybrid_rmse = [\n",
    "        garch_lstm_sp['performance_summary']['avg_test_rmse_realvol'],\n",
    "        garch_lstm_btc['performance_summary']['avg_test_rmse_realvol'],\n",
    "        garch_lstm_eur['performance_summary']['avg_test_rmse_realvol']\n",
    "    ]\n",
    "\n",
    "    hybrid_mae = [\n",
    "        garch_lstm_sp['performance_summary']['avg_test_mae_realvol'],\n",
    "        garch_lstm_btc['performance_summary']['avg_test_mae_realvol'],\n",
    "        garch_lstm_eur['performance_summary']['avg_test_mae_realvol']\n",
    "    ]\n",
    "\n",
    "    # Percentage improvement\n",
    "    rmse_improvement = [(g - h) / g * 100 for g, h in zip(garch_rmse, hybrid_rmse)]\n",
    "    mae_improvement  = [(g - h) / g * 100 for g, h in zip(garch_mae, hybrid_mae)]\n",
    "\n",
    "    x = np.arange(len(datasets))\n",
    "    width = 0.35  # bar width\n",
    "\n",
    "    # Plot Test RMSE\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "    ax0 = ax[0]\n",
    "    bars1 = ax0.bar(x - width/2, garch_rmse, width, label='GARCH', alpha=0.7)\n",
    "    bars2 = ax0.bar(x + width/2, hybrid_rmse, width, label='GARCH-LSTM', alpha=0.7)\n",
    "\n",
    "    ax0.set_title(\"Test RMSE Comparison (Lower is Better)\")\n",
    "    ax0.set_xticks(x)\n",
    "    ax0.set_xticklabels(datasets)\n",
    "    ax0.set_ylabel(\"RMSE\")\n",
    "    ax0.legend()\n",
    "    ax0.grid(axis='y', alpha=0.3)\n",
    "\n",
    "    # Annotate improvements\n",
    "    for i in range(len(x)):\n",
    "        ax0.text(x[i] + width/2, hybrid_rmse[i], f\"{rmse_improvement[i]:.1f}%\", \n",
    "                 ha='center', va='bottom', fontsize=10, color='green')\n",
    "\n",
    "    # Plot Test MAE\n",
    "    ax1 = ax[1]\n",
    "    bars3 = ax1.bar(x - width/2, garch_mae, width, label='GARCH', alpha=0.7)\n",
    "    bars4 = ax1.bar(x + width/2, hybrid_mae, width, label='GARCH-LSTM', alpha=0.7)\n",
    "\n",
    "    ax1.set_title(\"Test MAE Comparison (Lower is Better)\")\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(datasets)\n",
    "    ax1.set_ylabel(\"MAE\")\n",
    "    ax1.legend()\n",
    "    ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "    # Annotate improvements\n",
    "    for i in range(len(x)):\n",
    "        ax1.text(x[i] + width/2, hybrid_mae[i], f\"{mae_improvement[i]:.1f}%\", \n",
    "                 ha='center', va='bottom', fontsize=10, color='green')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_garch_vs_garch_lstm_bar(\n",
    "    garch_results_sp, garch_results_btc, garch_results_eur,\n",
    "    garch_lstm_sp, garch_lstm_btc, garch_lstm_eur\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fb0af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "garch_results_sp['performance_summary']['avg_test_rmse']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary table\n",
    "summary_data = {\n",
    "    'Model': ['GARCH', 'GARCH-LSTM', 'Improvement (%)'],\n",
    "    'S&P 500 Test RMSE': [\n",
    "        f'{garch_results_sp[\"performance_summary\"][\"avg_test_rmse\"]:.8f}',\n",
    "        f'{garch_lstm_sp[\"performance_summary\"][\"avg_test_rmse_realvol\"]:.8f}',\n",
    "        f'{((garch_results_sp[\"performance_summary\"][\"avg_test_rmse\"] - garch_lstm_sp[\"performance_summary\"][\"avg_test_rmse_realvol\"]) / garch_results_sp[\"performance_summary\"][\"avg_test_rmse\"] * 100):.2f}%'\n",
    "    ],\n",
    "    'S&P 500 Test MAE': [\n",
    "        f'{garch_results_sp[\"performance_summary\"][\"avg_test_mae\"]:.8f}',\n",
    "        f'{garch_lstm_sp[\"performance_summary\"][\"avg_test_mae_realvol\"]:.8f}',\n",
    "        f'{((garch_results_sp[\"performance_summary\"][\"avg_test_mae\"] - garch_lstm_sp[\"performance_summary\"][\"avg_test_mae_realvol\"]) / garch_results_sp[\"performance_summary\"][\"avg_test_mae\"] * 100):.2f}%'\n",
    "    ],\n",
    "    'Bitcoin Test RMSE': [\n",
    "        f'{garch_results_btc[\"performance_summary\"][\"avg_test_rmse\"]:.8f}',\n",
    "        f'{garch_lstm_btc[\"performance_summary\"][\"avg_test_rmse_realvol\"]:.8f}',\n",
    "        f'{((garch_results_btc[\"performance_summary\"][\"avg_test_rmse\"] - garch_lstm_btc[\"performance_summary\"][\"avg_test_rmse_realvol\"]) / garch_results_btc[\"performance_summary\"][\"avg_test_rmse\"] * 100):.2f}%'\n",
    "    ],\n",
    "    'Bitcoin Test MAE': [\n",
    "        f'{garch_results_btc[\"performance_summary\"][\"avg_test_mae\"]:.8f}',\n",
    "        f'{garch_lstm_btc[\"performance_summary\"][\"avg_test_mae_realvol\"]:.8f}',\n",
    "        f'{((garch_results_btc[\"performance_summary\"][\"avg_test_mae\"] - garch_lstm_btc[\"performance_summary\"][\"avg_test_mae_realvol\"]) / garch_results_btc[\"performance_summary\"][\"avg_test_mae\"] * 100):.2f}%'\n",
    "    ],\n",
    "    'EURUSD Test RMSE': [\n",
    "        f'{garch_results_eur[\"performance_summary\"][\"avg_test_rmse\"]:.8f}',\n",
    "        f'{garch_lstm_eur[\"performance_summary\"][\"avg_test_rmse_realvol\"]:.8f}',\n",
    "        f'{((garch_results_eur[\"performance_summary\"][\"avg_test_rmse\"] - garch_lstm_eur[\"performance_summary\"][\"avg_test_rmse_realvol\"]) / garch_results_eur[\"performance_summary\"][\"avg_test_rmse\"] * 100):.2f}%'\n",
    "    ],\n",
    "    'EURUSD Test MAE': [\n",
    "        f'{garch_results_eur[\"performance_summary\"][\"avg_test_mae\"]:.8f}',\n",
    "        f'{garch_lstm_eur[\"performance_summary\"][\"avg_test_mae_realvol\"]:.8f}',\n",
    "        f'{((garch_results_eur[\"performance_summary\"][\"avg_test_mae\"] - garch_lstm_eur[\"performance_summary\"][\"avg_test_mae_realvol\"]) / garch_results_eur[\"performance_summary\"][\"avg_test_mae\"] * 100):.2f}%'\n",
    "    ],\n",
    "}\n",
    "\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"GARCH-LSTM HYBRID MODEL: PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*100)\n",
    "print(summary_df.to_string(index=False))\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Save summary to CSV\n",
    "summary_df.to_csv('garch_lstm_performance_summary.csv', index=False)\n",
    "print(\"\\nPerformance summary saved as 'garch_lstm_performance_summary.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22b0590",
   "metadata": {},
   "source": [
    "# Get all predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1b8022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SP\n",
    "print(\"S&P 500\")\n",
    "garch_sp_predictions = garch_results_sp['all_results'][0]['test_forecast_vol']\n",
    "garch_sp_actual = sp500_clean['Log_Returns'].iloc[-len(garch_sp_predictions):]\n",
    "garch_lstm_sp_predictions = garch_lstm_sp['all_results'][0]['y_predictions']\n",
    "garch_lstm_sp_actual = garch_lstm_sp['all_results'][0]['y_actual']\n",
    "print(f'GARCH Predictions: {garch_sp_predictions}')\n",
    "print(f'GARCH-LSTM Predictions: {garch_lstm_sp_predictions}')\n",
    "print(f'Actual: {garch_lstm_sp_actual}')\n",
    "\n",
    "# BTC\n",
    "print(\"Bitcoin\")\n",
    "garch_btc_predictions = garch_results_btc['all_results'][0]['test_forecast_vol']\n",
    "garch_btc_actual = bitcoin_clean['Log_Returns'].iloc[-len(garch_btc_predictions):]\n",
    "garch_lstm_btc_predictions = garch_lstm_btc['all_results'][0]['y_predictions']\n",
    "garch_lstm_btc_actual = garch_lstm_btc['all_results'][0]['y_actual']\n",
    "print(f'GARCH Predictions: {garch_btc_predictions}')\n",
    "print(f'GARCH-LSTM Predictions: {garch_lstm_btc_predictions}')\n",
    "print(f'Actual: {garch_lstm_btc_actual}')\n",
    "\n",
    "# EURUSD\n",
    "print(\"EURUSD\")\n",
    "garch_eur_predictions = garch_results_eur['all_results'][0]['test_forecast_vol']\n",
    "garch_eur_actual = eurusd_clean['Log_Returns'].iloc[-len(garch_eur_predictions):]\n",
    "garch_lstm_eur_predictions = garch_lstm_eur['all_results'][0]['y_predictions']\n",
    "garch_lstm_eur_actual = garch_lstm_eur['all_results'][0]['y_actual']\n",
    "print(f'GARCH Predictions: {garch_eur_predictions}')\n",
    "print(f'GARCH-LSTM Predictions: {garch_lstm_eur_predictions}')\n",
    "print(f'Actual: {garch_lstm_eur_actual}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2753f222",
   "metadata": {},
   "source": [
    "# Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53d7c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def volatility_predictions_to_returns_new(predictions, actual_returns, cost=0.0):\n",
    "\n",
    "    min_len = min(len(predictions), len(actual_returns))\n",
    "    preds = np.asarray(predictions[:min_len]).reshape(-1)\n",
    "    rets  = (actual_returns.iloc[:min_len].values\n",
    "             if isinstance(actual_returns, pd.Series)\n",
    "             else np.asarray(actual_returns[:min_len]).reshape(-1))\n",
    "\n",
    "    med = np.median(preds)\n",
    "    dev = preds - med\n",
    "\n",
    "    signal = np.where(dev >  cost,  1,\n",
    "              np.where(dev < -cost, -1, 0))\n",
    "\n",
    "    if cost > 0:\n",
    "        changed = np.r_[0, np.diff(signal) != 0]  \n",
    "        trade_costs = changed * cost\n",
    "    else:\n",
    "        trade_costs = 0.0\n",
    "\n",
    "    strategy_returns = signal * rets - trade_costs\n",
    "\n",
    "    return pd.Series(strategy_returns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86561a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def annualized_return(daily_returns):\n",
    "    cumulative = (1 + daily_returns).prod()\n",
    "    n = daily_returns.shape[0]\n",
    "    return cumulative ** (TRADING_DAYS / n) - 1\n",
    "\n",
    "\n",
    "def annualized_std(daily_returns):\n",
    "    return daily_returns.std() * np.sqrt(TRADING_DAYS)\n",
    "\n",
    "\n",
    "def max_drawdown(daily_returns):\n",
    "    equity = (1 + daily_returns).cumprod()\n",
    "    peak = equity.cummax()\n",
    "    drawdown = (equity - peak) / peak\n",
    "    return np.abs(drawdown.min())  # Paper uses absolute value\n",
    "\n",
    "\n",
    "def information_ratio(strategy_returns, benchmark_returns):\n",
    "    arc = annualized_return(strategy_returns)\n",
    "    asd = annualized_std(strategy_returns)\n",
    "    \n",
    "    if asd == 0:\n",
    "        return np.nan\n",
    "    return arc / asd\n",
    "\n",
    "\n",
    "def modified_information_ratio(strategy_returns, benchmark_returns):\n",
    "    arc = annualized_return(strategy_returns)\n",
    "    asd = annualized_std(strategy_returns)\n",
    "    md = max_drawdown(strategy_returns)\n",
    "    \n",
    "    if asd == 0 or md == 0:\n",
    "        return np.nan\n",
    "        \n",
    "    return (arc * np.sign(arc) * arc) / (asd * md)\n",
    "\n",
    "\n",
    "def sortino_ratio(daily_returns, risk_free_rate=0):\n",
    "    negative_returns = daily_returns[daily_returns < 0]\n",
    "    \n",
    "    if len(negative_returns) == 0:\n",
    "        return np.nan\n",
    "        \n",
    "    # Calculate downside deviation (annualized)\n",
    "    downside_std = np.std(negative_returns, ddof=1)\n",
    "    asd_downside = downside_std * np.sqrt(TRADING_DAYS)\n",
    "    \n",
    "    arc = annualized_return(daily_returns)\n",
    "    \n",
    "    if asd_downside == 0:\n",
    "        return np.nan\n",
    "        \n",
    "    return arc / asd_downside\n",
    "\n",
    "\n",
    "def compute_performance_indicators(strategy_returns, benchmark_returns):\n",
    "    return {\n",
    "        \"ARC\": annualized_return(strategy_returns),\n",
    "        \"ASD\": annualized_std(strategy_returns),\n",
    "        \"MD\": abs(max_drawdown(strategy_returns)),\n",
    "        \"IR\": information_ratio(strategy_returns, benchmark_returns),\n",
    "        \"IR*\": modified_information_ratio(strategy_returns, benchmark_returns),\n",
    "        \"SR\": sortino_ratio(strategy_returns)\n",
    "    }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba2b68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRADING_DAYS = 252\n",
    "\n",
    "\n",
    "sp500_garch_strategy_returns = volatility_predictions_to_returns_new(\n",
    "    predictions=garch_sp_predictions,\n",
    "    actual_returns=garch_sp_actual,\n",
    "    cost=0.005  \n",
    ")\n",
    "\n",
    "sp500_hybrid_strategy_returns = volatility_predictions_to_returns_new(\n",
    "    predictions=garch_lstm_sp_predictions,\n",
    "    actual_returns=garch_lstm_sp_actual,\n",
    "    cost=0.005  \n",
    ")\n",
    "\n",
    "sp500_bnh_aligned = sp500_clean['Log_Returns'].values\n",
    "\n",
    "\n",
    "results_sp500 = []\n",
    "\n",
    "# GARCH\n",
    "garch_metrics = compute_performance_indicators(\n",
    "    pd.Series(sp500_garch_strategy_returns.squeeze()),\n",
    "    pd.Series(sp500_bnh_aligned.squeeze())\n",
    ")\n",
    "garch_metrics['Model'] = 'GARCH'\n",
    "garch_metrics['Num_Trades'] = int(np.sum(np.abs(np.diff(sp500_garch_strategy_returns > 0)) > 0))\n",
    "results_sp500.append(garch_metrics)\n",
    "\n",
    "TRADING_DAYS = 252\n",
    "# GARCH + LSTM\n",
    "hybrid_metrics = compute_performance_indicators(\n",
    "    pd.Series(sp500_hybrid_strategy_returns.squeeze()),\n",
    "    pd.Series(sp500_bnh_aligned.squeeze())\n",
    ")\n",
    "hybrid_metrics['Model'] = 'GARCH-LSTM'\n",
    "hybrid_metrics['Num_Trades'] = int(np.sum(np.abs(np.diff(sp500_hybrid_strategy_returns > 0)) > 0))\n",
    "results_sp500.append(hybrid_metrics)\n",
    "\n",
    "table2_sp500 = pd.DataFrame(results_sp500)\n",
    "\n",
    "print(\"TABLE: S&P 500 Long-Short Strategy Results\")\n",
    "print(table2_sp500[['Model', 'ARC', 'ASD', 'MD', 'IR', 'IR*', 'SR']].to_string(index=False))\n",
    "\n",
    "table2_sp500.to_csv('table2_sp500.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c964075",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRADING_DAYS = 365\n",
    "\n",
    "# Get benchmark returns (Buy-and-Hold)\n",
    "bitcoin_bnh_returns = bitcoin_clean['Log_Returns'].values\n",
    "\n",
    "bitcoin_garch_strategy_returns = volatility_predictions_to_returns_new(\n",
    "    predictions=garch_btc_predictions,\n",
    "    actual_returns=garch_btc_actual,\n",
    "    cost=0.01  \n",
    ")\n",
    "\n",
    "bitcoin_hybrid_strategy_returns = volatility_predictions_to_returns_new(\n",
    "    predictions=garch_lstm_btc_predictions,\n",
    "    actual_returns=garch_lstm_btc_actual,\n",
    "    cost=0.01\n",
    ")\n",
    "\n",
    "bitcoin_bnh_aligned = bitcoin_clean['Log_Returns'].values\n",
    "\n",
    "results_bitcoin = []\n",
    "\n",
    "# GARCH\n",
    "garch_metrics = compute_performance_indicators(\n",
    "    pd.Series(bitcoin_garch_strategy_returns.squeeze()),\n",
    "    pd.Series(bitcoin_bnh_aligned.squeeze())\n",
    ")\n",
    "garch_metrics['Model'] = 'GARCH'\n",
    "garch_metrics['Num_Trades'] = int(np.sum(np.abs(np.diff(bitcoin_garch_strategy_returns > 0)) > 0))\n",
    "results_bitcoin.append(garch_metrics)\n",
    "\n",
    "TRADING_DAYS = 365\n",
    "# GARCH + LSTM\n",
    "hybrid_metrics = compute_performance_indicators(\n",
    "    pd.Series(bitcoin_hybrid_strategy_returns.squeeze()),\n",
    "    pd.Series(bitcoin_bnh_aligned.squeeze())\n",
    ")\n",
    "hybrid_metrics['Model'] = 'GARCH-LSTM'\n",
    "hybrid_metrics['Num_Trades'] = int(np.sum(np.abs(np.diff(bitcoin_hybrid_strategy_returns > 0)) > 0))\n",
    "results_bitcoin.append(hybrid_metrics)\n",
    "\n",
    "table2_bitcoin = pd.DataFrame(results_bitcoin)\n",
    "\n",
    "\n",
    "print(\"TABLE: Bitcoin Long-Short Strategy Results\")\n",
    "print(table2_bitcoin[['Model', 'ARC', 'ASD', 'MD', 'IR', 'IR*', 'SR']].to_string(index=False))\n",
    "\n",
    "table2_bitcoin.to_csv('table2_bitcoin.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee22cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRADING_DAYS = 232\n",
    "\n",
    "# Get benchmark returns (Buy-and-Hold)\n",
    "eurusd_bnh_returns = eurusd_clean['Log_Returns'].values\n",
    "\n",
    "\n",
    "eurusd_garch_strategy_returns = volatility_predictions_to_returns_new(\n",
    "    predictions=garch_eur_predictions,\n",
    "    actual_returns=garch_eur_actual,\n",
    "    cost=0.0001  \n",
    ")\n",
    "\n",
    "eurusd_hybrid_strategy_returns = volatility_predictions_to_returns_new(\n",
    "    predictions=garch_lstm_eur_predictions,\n",
    "    actual_returns=garch_lstm_eur_actual,\n",
    "    cost=0.0001 \n",
    ")\n",
    "\n",
    "eurusd_bnh_aligned = eurusd_clean['Log_Returns'].values\n",
    "\n",
    "results_eurusd = []\n",
    "\n",
    "# GARCH\n",
    "garch_metrics = compute_performance_indicators(\n",
    "    pd.Series(eurusd_garch_strategy_returns.squeeze()),\n",
    "    pd.Series(eurusd_bnh_aligned.squeeze())\n",
    ")\n",
    "garch_metrics['Model'] = 'GARCH'\n",
    "garch_metrics['Num_Trades'] = int(np.sum(np.abs(np.diff(eurusd_garch_strategy_returns > 0)) > 0))\n",
    "results_eurusd.append(garch_metrics)\n",
    "\n",
    "TRADING_DAYS = 252\n",
    "# GARCH + LSTM\n",
    "hybrid_metrics = compute_performance_indicators(\n",
    "    pd.Series(eurusd_hybrid_strategy_returns.squeeze()),\n",
    "    pd.Series(eurusd_bnh_aligned.squeeze())\n",
    ")\n",
    "hybrid_metrics['Model'] = 'GARCH-LSTM'\n",
    "hybrid_metrics['Num_Trades'] = int(np.sum(np.abs(np.diff(eurusd_hybrid_strategy_returns > 0)) > 0))\n",
    "results_eurusd.append(hybrid_metrics)\n",
    "\n",
    "table2_eurusd = pd.DataFrame(results_eurusd)\n",
    "\n",
    "print(\"TABLE: EURUSD Long-Short Strategy Results\")\n",
    "print(table2_eurusd[['Model', 'ARC', 'ASD', 'MD', 'IR', 'IR*', 'SR']].to_string(index=False))\n",
    "\n",
    "table2_eurusd.to_csv('table2_eurusd.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
